[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome to my blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!"
  },
  {
    "objectID": "posts/blog-with-quarto/index.html",
    "href": "posts/blog-with-quarto/index.html",
    "title": "Създаване на блог с quarto",
    "section": "",
    "text": "Създаване на блог за техническа информация с използване на платформата quarto"
  },
  {
    "objectID": "posts/blog-with-quarto/index.html#характеристики",
    "href": "posts/blog-with-quarto/index.html#характеристики",
    "title": "Създаване на блог с quarto",
    "section": "Характеристики:",
    "text": "Характеристики:\n\nпроста и интуитивна предварително зададена структура\nподдържа md синтаксис за въвеждане на текст, заглавия и секции, връзки, картинки, таблици\nподдържа преобразуване на jupyter тетрадки, съдържащи код, формули, графики"
  },
  {
    "objectID": "posts/blog-with-quarto/index.html#основни-стъпки",
    "href": "posts/blog-with-quarto/index.html#основни-стъпки",
    "title": "Създаване на блог с quarto",
    "section": "Основни стъпки:",
    "text": "Основни стъпки:\n\nлокално инсталиране https://quarto.org/docs/get-started/\nгенериране на структурата https://quarto.org/docs/websites/website-blog.html:\n\nотваря се терминал от папката, където ще се съхранява блога и се изпълнява команда:\n quarto create-project myblog --type website:blog\nвизуализира се генерирания блог\n quarto preview myblog\n\nредактират се различните файлове за запознаване със структурата.\n\nЗаглавната страница е във файл index.qmd в коренната директория.\nОбща информация за блога се дава във файл about.qmd в коренната директория\nВсяко съобщение има папка в папка posts и файл index.qmd с основното съдържание. В папката може да има и картинки.\n\nизбира се тема за оформлението на сайта и се задава във файл _quarto.yml:\n\n     format:\n       html:\n         theme: litera\n         css: styles.css\nТемата по подразбиране е cosmo и е със синя заглавна лента. Темата, избрана за този блог е litera с по-убити цветове. Интересна тема е journal, както и много други (cerulean, cyborg, darkly, flatly, lumen, lux, materia, minty, morph, pulse, quartz, sandstone, simplex, sketchy, slate, solar, spacelab, superhero, united, vapor, yeti, zephyr)."
  },
  {
    "objectID": "posts/blog-with-quarto/index.html#публикуване-на-jupyter-тетрадки",
    "href": "posts/blog-with-quarto/index.html#публикуване-на-jupyter-тетрадки",
    "title": "Създаване на блог с quarto",
    "section": "Публикуване на Jupyter тетрадки",
    "text": "Публикуване на Jupyter тетрадки\n\nJust put the notebook in the “posts” folder\nAt the beginning of the jupyter notebook we need to add a cell of type raw with metadata, e.g.\n\n---\ntitle: \"Gradients, broadcasting and backpropagation\"\nauthor: \"me\"\ndate: \"2022-11-20\"\ncategories: [NN]\nformat:\n  html:\n    code-fold: false\njupyter: python3\n---"
  },
  {
    "objectID": "posts/loss/loss.html",
    "href": "posts/loss/loss.html",
    "title": "The Loss",
    "section": "",
    "text": "The loss function determines the output of the neural network. The output layer is not necessarily trained to be equal to the target.\nLet’ define \\(z\\) as the output of the last linear layer (no activation):\n\\(z = [z_1, \\dots, z_h, \\dots, z_H]\\)\nBefore the loss can be calculated some non-linear transformations may be needed. For example, a classification problem requires the output to be interpreted as a probability for the class assigned to the output node. Therefore the output is skewed to fit in the range [0, 1].\nIt should be noted that this need is not limited to the classification tasks. In a regression problem for predicting values with high dynamic ranges, the same error does not have the same effect for all predictions, e.g. prediction error of \\$1 for a product of \\$1000 is small and acceptable, but the same error when the product is \\$2 is not good. The simplest solution in such situation is to use logarithm, but this is out of the scope of this tutorial.\nThe standard loss functions are crossentropy (for classification) and mean squared error (for regression). The non-linearity could be added as a separate layer or could be part of the Loss calculation. This changes the actual output and loss."
  },
  {
    "objectID": "posts/loss/loss.html#logsoftmax-calculation",
    "href": "posts/loss/loss.html#logsoftmax-calculation",
    "title": "The Loss",
    "section": "Log(Softmax) calculation",
    "text": "Log(Softmax) calculation\n\ndef log_softmax(x):\n    '''Logarithm of predicted probabilities calculated from the output'''\n    return softmax(x).log()\n\n\ntest_close(log_softmax(z), torch.log_softmax(z, dim=-1))\n\n\ntest_close(log_softmax(zz), torch.log_softmax(zz, dim=-1))\n\n\ndef log_softmax2(x):\n    return x - x.exp().sum(dim=-1, keepdim=True).log()\n\n\ntest_close(log_softmax2(z), torch.log_softmax(z, dim=-1))\n\n\ntest_close(log_softmax2(zz), torch.log_softmax(zz, dim=-1))\n\n\ndef logsumexp(x):\n    # a = x.max(dim=-1, keepdim=True)[0]\n    # return a + (x-a).exp().sum(dim=-1, keepdim=True).log()\n    a = x.max(dim=-1)[0]\n    return a + (x-a[...,None]).exp().sum(dim=-1).log()\n\n\ntest_close(logsumexp(z), torch.logsumexp(z, dim=-1))\n\n\ntest_close(logsumexp(zz), torch.logsumexp(zz, dim=-1))\n\n\ndef log_softmax3(x):\n    return x - logsumexp(x).unsqueeze(-1)\n\n\ntest_close(log_softmax3(z), torch.log_softmax(z, dim=-1))\n\n\ntest_close(log_softmax3(zz), torch.log_softmax(zz, dim=-1))"
  },
  {
    "objectID": "posts/loss/loss.html#cross-entropy-loss-for-log-probabilities-f.nll_loss",
    "href": "posts/loss/loss.html#cross-entropy-loss-for-log-probabilities-f.nll_loss",
    "title": "The Loss",
    "section": "Cross-entropy loss for log-probabilities, F.nll_loss()",
    "text": "Cross-entropy loss for log-probabilities, F.nll_loss()\n\\(\\mathbb{L} = - \\sum_{i=1}^N \\sum_{h=1}^H{y_{ij} \\ln(p(c_{ih})) } = - \\sum_{i=1}^N {\\ln(\\hat{y}_{i}) }\\)\nIf case of training with such a loss function, the output of the network should be interpreted as log-probabilities. To convert to probabilities, take the exponent of the predictions. My note: a kind of failure intensity: \\(p = \\exp(-\\lambda t) \\implies \\ln(p) = -\\lambda t\\) compare with \\(q = 1 - p = 1 - \\exp(-\\lambda t) \\approx \\lambda t\\).\n\ndef nll(x, y):\n    '''Take the mean value of the correct x\n       x: pred_as_log_softmax\n       y: target_as_index\n    '''\n    N = y.shape[0]\n    loss = -x[range(N), y].mean()\n    return loss\n\n\nimport torch.nn.functional as F\n\n\nvv = -torch.rand((6, 2))*5\nnll(vv, y)\n\ntensor(2.1238)\n\n\n\n# notice that the log-likelihood values are negative!\nvv, y, [c[yi] for yi in y]\n\n(tensor([[-2.1895, -1.1382],\n         [-1.5560, -4.0288],\n         [-1.4380, -1.8596],\n         [-1.7396, -1.1178],\n         [-4.1428, -3.8919],\n         [-0.0845, -2.6688]]),\n tensor([1, 1, 1, 0, 1, 0]),\n ['female', 'female', 'female', 'male', 'female', 'male'])\n\n\n\ntest_close(nll(vv,y), F.nll_loss(vv, y))\n\n\n# explore the correspondance between log-likelihoods and linelihoods\na = tensor([-4, -3, -2, -1, -0.5, -0.2, -0.05, 0, 0.5])\naa = a.exp()\nprint(['%.2f' %i for i in a])\nprint(['%.3f' %i for i in aa])\n\n['-4.00', '-3.00', '-2.00', '-1.00', '-0.50', '-0.20', '-0.05', '0.00', '0.50']\n['0.018', '0.050', '0.135', '0.368', '0.607', '0.819', '0.951', '1.000', '1.649']"
  },
  {
    "objectID": "posts/loss/loss.html#cross-entropy-loss-for-raw-outputs-f.cross_entropy",
    "href": "posts/loss/loss.html#cross-entropy-loss-for-raw-outputs-f.cross_entropy",
    "title": "The Loss",
    "section": "Cross-entropy loss for raw outputs, F.cross_entropy()",
    "text": "Cross-entropy loss for raw outputs, F.cross_entropy()\nCross-entropy is calculated directly from the output without conversion to probabilities or log-probabilities. All these operations are included in the loss function calculation. The interpretation of the output is unclear, could be any value from \\(-\\infty\\) to \\(\\infty\\) – useful for other regression tasks\n\ndef cross_entropy(x, y):\n    return nll(log_softmax3(x), y)\n\n\ncross_entropy(zz, y)\n\ntensor(1.3343)\n\n\n\ntest_close(cross_entropy(zz, y), F.cross_entropy(zz, y))"
  },
  {
    "objectID": "posts/loss/loss.html#cross-entropy-loss-for-multi-label-target",
    "href": "posts/loss/loss.html#cross-entropy-loss-for-multi-label-target",
    "title": "The Loss",
    "section": "Cross-entropy loss for multi-label target",
    "text": "Cross-entropy loss for multi-label target\n\\(\\mathbb{L} = - \\sum_{i=1}^N \\sum_{h=1}^H{[ y_{ih} \\ln(\\hat{y_{ih}}) + (1 -y_{ih}) \\ln(1 -\\hat{y}_{ih})]}\\)\n\\(\\mathbb{L} = - \\sum_{i=1}^N \\sum_{h=1}^H{[ y_{ih} \\ln(p(c_{ih})) + (1 -y_{ih}) \\ln(1 -p(c_{ih}))]}\\)\n\\(\\mathbb{L} = - \\sum_{i=1}^N \\sum_{h=1}^H \\sum_{k=1}^2{y_{ihk} \\ln(p(c_{ih}[k])) }\\)\n\\(\\mathbb{L} = - \\sum_{i=1}^N{[ y_i \\ln(\\mathrm{sigmoid(z_i)}) + (1 -y_i) \\ln(1 -\\mathrm{sigmoid(z_i)})]}\\)\n\n# Generate test targets - a kind of one hot encoded but multiple ones are allowed\nyy[0, 0] = 1\nprint('Both ones for the first sample:\\n', yy)\n\nBoth ones for the first sample:\n tensor([[1., 1.],\n        [0., 1.],\n        [0., 1.],\n        [1., 0.],\n        [0., 1.],\n        [1., 0.]])\n\n\n\ndef cross_entropy_multi(z, y):\n    '''z: pred as unnormalized scores\n       y: target as binary encoded aray'''\n    s = torch.sigmoid(z)\n    loss = -(y * s.log() + (1 - y)*(1 - s).log()).mean()\n    return loss\n\n\ncross_entropy_multi(zz, yy)\n\ntensor(1.2954)\n\n\n\nF.binary_cross_entropy_with_logits(zz, yy)\n\ntensor(1.2954)\n\n\n\nF.binary_cross_entropy(torch.sigmoid(zz), yy)\n\ntensor(1.2954)\n\n\nTO DO: clarify notations (not consistent yet!), add visualizations"
  },
  {
    "objectID": "posts/gradients/gradients.html",
    "href": "posts/gradients/gradients.html",
    "title": "Gradients, broadcasting and backpropagation",
    "section": "",
    "text": "Let’s have \\(y = w \\cdot x + b\\). Application of the rules of differentiation is simple, e.g.\n\\[\\frac{\\mathrm{d}y}{\\mathrm{d}x} = w , \\quad\n\\frac{\\mathrm{d}y}{\\mathrm{d}w} = x, \\quad\n\\frac{\\mathrm{d}y}{\\mathrm{d}b} = 1\\]\nThe change in \\(y\\) is porportional to the change in \\(x\\). The bigger is \\(w\\), the bigger is the change of \\(y\\) for the same change of \\(x\\).\nLet’s \\(L = f(y) = f(y(x))\\). Application of the chain rule is simple, e.g.\n\\[\\frac{\\mathrm{d}L}{\\mathrm{d}x} = \\frac{\\mathrm{d}f(y)}{\\mathrm{d}y} \\cdot \\frac{\\mathrm{d}y}{\\mathrm{d}x} = \\frac{\\mathrm{d}L}{\\mathrm{d}y} \\cdot w\\] \\[\\frac{\\mathrm{d}L}{\\mathrm{d}w} = \\frac{\\mathrm{d}f(y)}{\\mathrm{d}y} \\cdot \\frac{\\mathrm{d}y}{\\mathrm{d}w} = \\frac{\\mathrm{d}L}{\\mathrm{d}y} \\cdot x\\] \\[\\frac{\\mathrm{d}L}{\\mathrm{d}b} = \\frac{\\mathrm{d}f(y)}{\\mathrm{d}y} \\cdot \\frac{\\mathrm{d}y}{\\mathrm{d}b} = \\frac{\\mathrm{d}L}{\\mathrm{d}y} \\cdot 1\\]\nThe multidimensional case is not so simple. Functions with multiple inputs and multiple outputs have multiple partial derivatives which need to be arranged and stored properly. Applying this for batches of data complicates the picture even more."
  },
  {
    "objectID": "posts/gradients/gradients.html#derivative-of-a-transformation",
    "href": "posts/gradients/gradients.html#derivative-of-a-transformation",
    "title": "Gradients, broadcasting and backpropagation",
    "section": "Derivative of a transformation",
    "text": "Derivative of a transformation\nThe derivative of a function (transformation \\(\\psi\\)) with multiple inputs \\(\\mathbf{x} \\in \\mathbb{R}^M\\) and multiple outputs \\(\\mathbf{y} \\in \\mathbb{R}^H\\) is a matrix containing the partial derivatives of each output with respect to each input (the so called Jacobian of the transformation, \\(\\frac{\\partial{\\mathbf{y}}}{\\partial{\\mathbf{x}}} \\in \\mathbb{R}^{M \\times H}\\)). For example, if \\(M=4\\) and \\(H=2\\) we can write:\n\\(\\mathbf{y} = \\psi(\\mathbf{x})\\),\n\\(\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4\\end{bmatrix}, \\quad \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix}\\)\n\\(\\frac{\\partial{\\mathbf{y}}}{\\partial{\\mathbf{x}}} = \\begin{bmatrix} \\frac{\\partial{y_1}}{\\partial{x_1}} & \\frac{\\partial{y_1}}{\\partial{x_2}} & \\frac{\\partial{y_1}}{\\partial{x_3}} & \\frac{\\partial{y_1}}{\\partial{x_4}}\\\\ \\frac{\\partial{y_2}}{\\partial{x_1}} & \\frac{\\partial{y_2}}{\\partial{x_2}} & \\frac{\\partial{y_2}}{\\partial{x_3}} & \\frac{\\partial{y_2}}{\\partial{x_4}} \\end{bmatrix}\\)\nThe derivative of \\(\\mathbf{y = W \\cdot x}\\) with respect to \\(\\mathbf{x}\\) is \\(\\mathbf{W}\\), i.e. \\(\\frac{∂y}{∂x} = W\\).\nWe should note that in neural networks the input and output features are arranged as raw vectors.\nThe derivative of \\(\\mathbf{y = x \\cdot W}\\) with respect to \\(\\mathbf{x}\\) is \\(\\mathbf{W^T}\\), i.e. \n\\[\\frac{∂y}{∂x} = W^T\\]"
  },
  {
    "objectID": "posts/gradients/gradients.html#the-chain-rule",
    "href": "posts/gradients/gradients.html#the-chain-rule",
    "title": "Gradients, broadcasting and backpropagation",
    "section": "The chain rule",
    "text": "The chain rule\nLet’s have \\(y = x \\cdot W + b\\) and \\(L = f(y)\\)\n\n\n\n\n\n\nSimple NN\n\n\n\n\n\n\n\nMatrix view, xW\n\n\n\n\n\nThe chain rule involves propagating the gradient of the loss layer after layer backward towards the inputs and parameters of interest. In our demonstration case in order to calculate the gradient of \\(L\\) with respect to the input \\(x\\) we need to have the gradient of \\(L\\) with respect to the output \\(y\\).\nThe shapes of the gradient is the same as the shape of the corresponding variable (parameter)\nA gradient is attached to each variable and parameter of the model, i.e.\n\\(y.g = \\frac{\\partial{L}}{∂{y}}\\)\n\\(x.g = \\frac{\\partial{L}}{∂{x}} = \\frac{\\partial{L}}{\\partial{y}} \\cdot \\frac{\\partial{y}}{\\partial{x}} = y.g \\cdot \\frac{\\partial{y}}{\\partial{x}} = y.g \\cdot W^T\\)\n\\(b.g = \\frac{\\partial{L}}{∂{b}} = \\frac{\\partial{L}}{\\partial{y}} \\cdot \\frac{\\partial{y}}{\\partial{b}} = y.g \\cdot \\frac{\\partial{y}}{\\partial{b}} = y.g\\)\n\\(W.g = \\frac{\\partial{L}}{∂W} = ((\\frac{\\partial{L}}{\\partial{y}})^T \\cdot \\frac{\\partial{y}}{\\partial{W}})^T = (y.g^T \\cdot x)^T = x^T \\cdot y.g\\)\n \nNotes:\n\n\\(x\\), \\(y\\) and \\(b\\) are row vectors.\n\\(W\\) is a weight matrix with \\(m\\) rows and \\(h\\) columns;\n\\(x\\) includes the \\(m\\) input features\n\\(b\\) is a bias with \\(h\\) elements;\n\\(y\\) has \\(h\\) features (or nodes).\n\\(x\\) and \\(y\\) represent input and output features (variables, nodes in the NN). Adding additional dimension (multiple rows) could represent multiple data samples. Inputs and outputs could be replaced by matrices \\(X\\) and \\(Y\\) where the last dimension gives the features (\\(x\\) and \\(y\\) for the corresponding data point);"
  },
  {
    "objectID": "posts/gradients/gradients.html#implementation-and-testing-with-code",
    "href": "posts/gradients/gradients.html#implementation-and-testing-with-code",
    "title": "Gradients, broadcasting and backpropagation",
    "section": "Implementation and testing with code",
    "text": "Implementation and testing with code\nAbove equations are implemented in function lin_grad and tested in the following code\n\ndef lin_grad(x, w, b, y):\n    # y.g shoudl be available!\n    b.g = y.g.sum(dim=0)#/y.shape[0]\n    w.g = x.T @ y.g\n    x.g = y.g @ w.T\n\n\nimport torch\nfrom torch import tensor\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n\n# Generate test input and ouput data\nN = 100 # Number of samples\nM = 4 # Number of input features\nH = 2 # Number of outputs\nx = torch.rand((N, M))*10 - 5 # Input\nk1, k2, k3, k4, k6, k7 = 1, 1.5, 2, 2.5, 3.0, -0.5\nW_true = torch.tensor([[k1, k1],\n                  [k2, k2],\n                  [k3, k6],\n                  [k4, k7]])\nb_true = torch.tensor([0, -1])\ny = x @ W_true + b_true # Output\nW_true, y[:7]\n\n(tensor([[ 1.0000,  1.0000],\n         [ 1.5000,  1.5000],\n         [ 2.0000,  3.0000],\n         [ 2.5000, -0.5000]]),\n tensor([[-11.5291, -12.7022],\n         [ -3.9763, -22.8349],\n         [  2.2930,   7.2452],\n         [ -4.9708,   6.3649],\n         [-11.7199, -24.9706],\n         [ 14.8387,   5.4946],\n         [ -9.7971,  -0.4925]]))\n\n\n\nplt.scatter(x[:,2], y[:,1]);\nplt.xlabel('Input, dimension (feature) No3');\nplt.ylabel('Output, dim. No2');\n\n\n\n\n\n# Generate random weights\nw = torch.randn(M,H)\nb = torch.zeros(H)\nw, b\n\n(tensor([[-0.0383, -0.8173],\n         [ 0.8458, -2.0662],\n         [ 0.1153,  0.0775],\n         [ 1.0845, -0.1016]]),\n tensor([0., 0.]))\n\n\nIn order to test the function lin_grad we need to calculate the gradient \\(∂L/dy\\) and save it in y_pred.g (the gradient with respect to the prediction):\n\ndefine model and calculate prediction\ndefine and calculate loss \\(L\\) as simple mean squared error mse(y_pred, y_targ)\ndefine and run mse_grad(y_pred, y_targ)\nrun lin_grad(x, w, b, y)\n\n\ndef lin(x, w, b): return x @ w + b\n\n\ny_pred = lin(x, w, b)\ny_pred[:5]\n\ntensor([[ 0.0472, -0.5698],\n        [ 1.4009,  5.7816],\n        [-3.4003,  1.4367],\n        [-0.9559, -5.0894],\n        [-0.9125,  9.5171]])\n\n\n\ndef mse(y_pred, y_targ): return (y_pred-y_targ).pow(2).mean()\n\n\nloss = mse(y_pred, y)\nloss\n\ntensor(181.8108)\n\n\n\ndef mse_grad(y_pred, y_targ): y_pred.g = 2 * (y_pred - y_targ) / y_targ.shape[0] / y_targ.shape[1]\n\n\nmse_grad(y_pred, y)\n\nFinnally, test if all dimensions in lin_grad match:\n\nlin_grad(x, w, b, y_pred)\n\nNext test if the loss improves\n\nw -= 0.1 * w.g\nb -= 0.1 * b.g\ny_pred = lin(x, w, b)\nloss = mse(y_pred, y)\nloss\n\ntensor(9.2100)\n\n\n\nfor i in range(20):\n    y_pred = lin(x, w, b)\n    loss = mse(y_pred, y)\n    mse_grad(y_pred, y)\n    lin_grad(x, w, b, y_pred)\n    w -= 0.01 * w.g\n    b -= 0.01 * b.g\n    print(loss, end=', ')\nprint('End!\\n')\nprint('Weights and biases of the network and for the dataset:')\nw, b, W_true, b_true\n\ntensor(9.2100), tensor(7.3055), tensor(5.8198), tensor(4.6593), tensor(3.7515), tensor(3.0402), tensor(2.4818), tensor(2.0425), tensor(1.6961), tensor(1.4221), tensor(1.2048), tensor(1.0318), tensor(0.8936), tensor(0.7827), tensor(0.6933), tensor(0.6208), tensor(0.5616), tensor(0.5130), tensor(0.4729), tensor(0.4394), End!\n\nWeights and biases of the network and for the dataset:\n\n\n(tensor([[ 1.0095,  0.9741],\n         [ 1.5336,  1.5728],\n         [ 1.9631,  3.0287],\n         [ 2.3814, -0.5464]]),\n tensor([ 0.0677, -0.2069]),\n tensor([[ 1.0000,  1.0000],\n         [ 1.5000,  1.5000],\n         [ 2.0000,  3.0000],\n         [ 2.5000, -0.5000]]),\n tensor([ 0, -1]))"
  },
  {
    "objectID": "posts/gradients/gradients.html#pytorch-autograd-and-backpropagation",
    "href": "posts/gradients/gradients.html#pytorch-autograd-and-backpropagation",
    "title": "Gradients, broadcasting and backpropagation",
    "section": "PyTorch autograd and backpropagation",
    "text": "PyTorch autograd and backpropagation\nWe will use PyTorch automatic gradient calculation to check our algorithms. This involves using the build-in methods and parameters .backward() and .grad.\nIn order to apply PyTorch backpropagation and autograd we need to define a forward function that relates the inputs with the loss:\n\ndef forward(x, y):\n    y_pred = lin(x, w, b)\n    loss =  mse(y_pred, y)\n    return loss\n\n\nloss = forward(x, y)\nloss\n\ntensor(0.4113)\n\n\nBut this is not enough:\n    loss.backward()\n–> RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\nLooks like a slot for saving the gradient to the corresponding inputs and model parameters should be required.\n\n# Update gradients as above - manual backward - for comparison\n\n# forward\ny_pred = lin(x, w, b)\nloss =  mse(y_pred, y)\n\n# backward\nmse_grad(y_pred, y)\nlin_grad(x, w, b, y_pred)\n\n# Good to know: the parameters are not updated in the backward pass!\n\n\n# Require gradient to keep them with the data\n\nfor element in [x, w, b]:\n    element.requires_grad_(True)\n\n\nloss = forward(x, y)\nloss.backward()\n\n\nloss\n\ntensor(0.4113, grad_fn=<MeanBackward0>)\n\n\n\nfrom fastcore.test import test_close\n\n\ntest_close(x.g, x.grad)\ntest_close(w.g, w.grad)\ntest_close(b.g, b.grad)"
  },
  {
    "objectID": "posts/gradients/gradients.html#what-is-next",
    "href": "posts/gradients/gradients.html#what-is-next",
    "title": "Gradients, broadcasting and backpropagation",
    "section": "What is next?",
    "text": "What is next?\nThe next step is to try to create a “proper” non-linear neural network.\nThe structure of a fully connected neural network with single hidden layer could be represented as follows:\n\n\n\nNN with one hidden layer\n\n\nA few more gradients need to be defined and calculated"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My notes and clarifications for the fast.ai course 2022p2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Neural Networks",
    "section": "",
    "text": "Hiding the details - the Module class\n\n\n\n\n\n\n\nNN\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nJan 3, 2023\n\n\nKrasin Georgiev\n\n\n\n\n\n\n  \n\n\n\n\nThe Loss\n\n\n\n\n\n\n\nNN\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2022\n\n\nKrasin Georgiev\n\n\n\n\n\n\n  \n\n\n\n\nGradients, broadcasting and backpropagation\n\n\n\n\n\n\n\nNN\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\nKrasin Georgiev\n\n\n\n\n\n\n  \n\n\n\n\nСъздаване на блог с quarto\n\n\n\n\n\n\n\nwebtools\n\n\nbg\n\n\n\n\n\n\n\n\n\n\n\nNov 18, 2022\n\n\nme\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to my blog\n\n\n\n\n\n\n\nadmin\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/classes/classes.html",
    "href": "posts/classes/classes.html",
    "title": "Hiding the details - the Module class",
    "section": "",
    "text": "How to build a NN model? When you drive a car you don’t see all car and engine parameters. Even when an engineer designs a car he don’t know details about each component (mechanical, electrical, electro-mechanical or electronic part). This is done to manage the complexity. This approach is especially important for software design where the number of components is much greater and design flexibility is colse to infinity.\nThe code was adapted from fastai course 2022, part 2 while studying notebooks 03_backprop.ipynb and 04_minibatch_training.ipynb.\n\nLayers\nSome of the most widely used layers in a NN are Linear, ReLU and Mse.\nThe weights of the layer are initiated when the object is created and are defined in the __init__ method. The relationship for calculating the output from the input are defined in the __call__ layer (the “forward” feature). The functionality for calculation of the gradients is provided in the backward method. For example:\n\nclass Lin():\n    def __init__(self, w, b):\n        self.w = w\n        self.b = b\n    def __call__(self, x):\n        y = x @ self.w + self.b\n        self.x = x\n        self.y = y\n        return self.y\n    def backward(self):        \n        self.b.g = self.y.g.sum(dim=0)\n        self.w.g = self.x.T @ self.y.g\n        self.x.g =  self.y.g @ self.w.T\n\n\nclass ReLU():\n    def __call__(self, x):\n        self.x = x\n        self.y = self.relu(x)\n        return self.y\n    def relu(self, x): return x.clamp_min(0.)\n    def backward(self): self.x.g = self.y.g * (self.x > 0)\n\n\nclass Mse():\n    def __call__(self, pred, y):\n        self.pred = pred\n        self.targ = y\n        return (pred-y).pow(2).mean()\n    def backward(self):\n        self.pred.g = 2 * (self.pred - self.targ) / self.targ.shape[0] / self.targ.shape[1]\n\nThe following code demonstrates the creation and use of a linear layer object. Look for:\n\nInitialization: lin = Lin(w, b\nForward application: y = lin(x)\nBackpropagation: lin.backward()\n\n\nimport torch\nfrom fastcore.test import test_close\n\n\n# Generate random weights for the model\nM = 4 # Number of input features\nH = 2 # Number of outputs\nw = torch.randn(M,H)\nb = torch.zeros(H)\n\n# Create linear layer object\nlin = Lin(w, b)\n\n# Simulate inputs\nN = 3 # Number of samples\nx = torch.rand((N, M))*10 - 5 # just random numbers as example\n\n# Calculate the output of the layer\ny = lin(x)\n\n# Show inputs and outputs\nprint('x = \\n', x, '\\ny = \\n', y)\n\nx = \n tensor([[ 3.6329, -1.5028,  1.5645, -2.5170],\n        [ 4.4755,  3.9404, -3.6032, -0.0517],\n        [ 1.0234, -3.7332,  3.1473,  0.6829]]) \ny = \n tensor([[ 0.6308, -2.3627],\n        [ 4.0016, -2.5262],\n        [-1.7378,  1.8884]])\n\n\n\n# Provide output gradients (usually based on the Loss: y.g = dL/dy)\ny.g = torch.rand(y.shape) # output gradients are needed in order for backpropagation to work\n\n# Calculate (backpropagate) gradients\nlin.backward()\n\n# Show gradients\nprint('x.g = \\n', x.g, '\\nw.g = \\n', w.g, '\\nb.g = \\n', b)\n\nx.g = \n tensor([[0.2319, 0.9382, 0.8166, 0.7303],\n        [0.2856, 0.8208, 0.5457, 0.4417],\n        [0.2376, 0.8698, 0.7110, 0.6232]]) \nw.g = \n tensor([[ 8.1594,  5.1629],\n        [-0.9825, -1.6451],\n        [ 0.8388,  1.4778],\n        [-1.6999, -1.4052]]) \nb.g = \n tensor([0., 0.])\n\n\n\nrelu = ReLU()\ny_relu = relu(x)\nx, y_relu\n\n(tensor([[ 3.6329, -1.5028,  1.5645, -2.5170],\n         [ 4.4755,  3.9404, -3.6032, -0.0517],\n         [ 1.0234, -3.7332,  3.1473,  0.6829]]),\n tensor([[3.6329, 0.0000, 1.5645, 0.0000],\n         [4.4755, 3.9404, 0.0000, 0.0000],\n         [1.0234, 0.0000, 3.1473, 0.6829]]))\n\n\n\ny_relu.g = torch.rand(y_relu.shape)\nrelu.backward()\nx_relu_g = x.g\n\n\n\nModels\nA model is an arrangement of one or more layers. It contains all the weights and relationships that allow an input to be transformed into an output and loss and the derivative of the loss to be back-propagated to the model weights and inputs. Each layer can be considered a simple model. Each model have the same general methods as a layer.\nA kind of a distinction between a model and layer could be introduced if the explicit output of the model is restricted to be a scalar - the loss. But the loss is quite often calculated separately over the outputs of the model in which case the models are just more complex layers.\nThe simplest arrangement is a sequence of layers where the output of each layer (except the last one) is input to the next layer.\n\n# A model with just a Linear, ReLU and Loss layers\n\n# The output of the model is the loss. \n# In additon, the output of the last layer (usually the loss is not counted as a layer)\n# is saved as model attribure self.y\n\nclass Model():\n    def __init__(self, w, b):\n        self.layers = [Lin(w, b), ReLU()]\n        self.loss = Mse()\n        \n    def __call__(self, x, targ):\n        for l in self.layers: \n            x = l(x)\n        self.y = x\n        return self.loss(x, targ)\n    \n    def backward(self):\n        self.loss.backward()\n        for l in reversed(self.layers): \n            l.backward()\n\n\nmodel = Model(w, b)\n\n\n# Simulate target values (needed to calculate the loss)\nk1, k2, k3, k4, k6, k7 = 1, 1.5, 2, 2.5, 3.0, -0.5\nW_true = torch.tensor([[k1, k1],\n                  [k2, k2],\n                  [k3, k6],\n                  [k4, k7]])\nb_true = torch.tensor([0, -1])\ny_target = x @ W_true + b_true # Output\n\n\nloss = model(x, y_target)\nloss\n\ntensor(10.5302)\n\n\n\nout = model.y\nout\n\ntensor([[0.6308, 0.0000],\n        [4.0016, 0.0000],\n        [0.0000, 1.8884]])\n\n\n\nmodel.backward()\n\n\n# Show gradients\nprint('x.g = \\n', x.g, '\\nw.g = \\n', w.g, '\\nb.g = \\n', b)\n\nx.g = \n tensor([[ 0.3201,  0.5563,  0.1117, -0.0025],\n        [ 0.1260,  0.2190,  0.0440, -0.0010],\n        [ 0.0929, -0.2466, -0.5286, -0.5591]]) \nw.g = \n tensor([[ 4.3441, -0.5581],\n        [ 0.0389,  2.0357],\n        [ 0.1176, -1.7162],\n        [-2.0432, -0.3724]]) \nb.g = \n tensor([0., 0.])\n\n\n\n\nThe Module class and class inheritance\nAbove classes can be based on a more general class so and more information can be hidden, e.g. saving parameter gradients, saving inputs and outputs, etc. Only the initialization and the functions needed to do the forward and backward pass need to be redefined. All we have is modules and submodules.\n\nclass Module():        \n    def __call__(self, *x):\n        self.x = x\n        self.y = self.forward(*x)\n        return self.y    \n    def backward(self):\n        self.bwd(self.y, *self.x)\n    def forward(self):\n        raise Exception('Not implemented')    \n    def bwd(self):\n        raise Exception('Not implemented')\n\n\nclass Lin(Module):\n    def __init__(self, w, b):\n        self.w = w\n        self.b = b\n    def forward(self, x):\n        y = x @ self.w + self.b\n        return y\n    def bwd(self, y, x):        \n        self.b.g = y.g.sum(dim=0)\n        self.w.g = x.T @ y.g\n        x.g =  y.g @ self.w.T\n\n\n# Create linear layer object\nlin = Lin(w, b)\n\n# Calculate the output of the layer\ny2 = lin(x)\n\n# Test results\ntest_close(y, y2)\n\n\nlin.y.g = torch.rand(lin.y.shape)\nlin.backward()\n\n\ntest_close(x.g, lin.x[0].g)\ntest_close(w.g, lin.w.g)\ntest_close(b.g, lin.b.g)\n\n\nclass ReLU(Module):\n    def forward(self, x): return x.clamp_min(0.)\n    def bwd(self, y, x): x.g = y.g * (x > 0)\n\n\nrelu = ReLU()\ntest_close( relu(x), y_relu)\n\n\nrelu.y.g = y_relu.g\nrelu.backward()\ntest_close(relu.x[0].g, x_relu_g)\n\n\nclass Mse(Module):\n    def forward(self, pred, y): return (pred-y).pow(2).mean()\n    def bwd(self, out, pred, targ):\n        pred.g = 2 * (pred - targ) / targ.shape[0] / targ.shape[1]\n\n\nmodel = Model(w, b)\n\n\ntest_close(model(x, y_target), loss)\n\n\nmodel.backward()\n\n\n# Show gradients\nprint('x.g = \\n', x.g, '\\nw.g = \\n', w.g, '\\nb.g = \\n', b)\n\nx.g = \n tensor([[ 0.3201,  0.5563,  0.1117, -0.0025],\n        [ 0.1260,  0.2190,  0.0440, -0.0010],\n        [ 0.0929, -0.2466, -0.5286, -0.5591]]) \nw.g = \n tensor([[ 4.3441, -0.5581],\n        [ 0.0389,  2.0357],\n        [ 0.1176, -1.7162],\n        [-2.0432, -0.3724]]) \nb.g = \n tensor([0., 0.])\n\n\n\n\nPyTorch layers and Module class\nThe above Module will be replaced by the standard PyTorch class nn.Module. The autograd and backpropagation features of PyTorch will be used to remove the need for defining the bwd or the backwardmethods. The Lin layer will be redefined to inherit nn.Module. The ReLU() and Mse() layers will be replaced by nn.ReLU() and nn.MSELoss() respectively.\n\nfrom torch import nn\n\n\nclass Lin(nn.Module):\n    def __init__(self, w, b):\n        super().__init__()\n        self.w = w.requires_grad_(True)\n        self.b = b.requires_grad_(True)\n    def forward(self, x):\n        y = x @ self.w + self.b\n        return y\n\n\n# Create linear layer object\nlin = Lin(w, b)\n\n# Calculate the output of the layer\ny2 = lin(x)\n\n# Test results\ntest_close(y, y2)\n\n\ny2.grad = torch.rand(y.shape) # output gradients are needed in order for backpropagation to work\n\n# Calculate (backpropagate) gradients\n# y2.backward()\n\n# RuntimeError: grad can be implicitly created only for scalar outputs\n\n\nclass Model(nn.Module):\n    def __init__(self, w, b):\n        super().__init__()\n        self.layers = [Lin(w, b), nn.ReLU()]\n        self.loss = nn.MSELoss()        \n    def forward(self, x, targ):\n        for l in self.layers: \n            x = l(x)\n        self.y = x\n        return self.loss(x, targ)\n\n\nmodel = Model(w, b)\n\n\nloss = model(x, y_target)\n\n\nloss\n\ntensor(10.5302, grad_fn=<MseLossBackward0>)\n\n\n\nloss.backward()\n\n\ntest_close(w.grad, w.g)\n\n\ntest_close(b.grad, b.g)\n\n\nx.grad\n# None because `x.requires_grad_(True)` was never called\n\n\nx.requires_grad_(True)\n\ntensor([[ 3.6329, -1.5028,  1.5645, -2.5170],\n        [ 4.4755,  3.9404, -3.6032, -0.0517],\n        [ 1.0234, -3.7332,  3.1473,  0.6829]], requires_grad=True)\n\n\n\n# gradients accumulate\nw.grad.zero_()\nb.grad.zero_()\n\ntensor([0., 0.])\n\n\n\nloss = model(x, y_target)\n\n\nloss.backward()\n\n\nx.grad\n\ntensor([[ 0.3201,  0.5563,  0.1117, -0.0025],\n        [ 0.1260,  0.2190,  0.0440, -0.0010],\n        [ 0.0929, -0.2466, -0.5286, -0.5591]])\n\n\n\ntest_close(x.grad, x.g)\n\n\ntest_close(w.grad, w.g)\n\n\n\nSome more details in PyTorch\n\n# notice that the layers of the model are not properly registered and accessible\nmodel\n\nModel(\n  (loss): MSELoss()\n)\n\n\n\nmodel.layers[1](x)\n\ntensor([[3.6329, 0.0000, 1.5645, 0.0000],\n        [4.4755, 3.9404, 0.0000, 0.0000],\n        [1.0234, 0.0000, 3.1473, 0.6829]], grad_fn=<ReluBackward0>)\n\n\n\nlist(model.named_children())\n\n[('loss', MSELoss())]\n\n\n\nlist(model.parameters())\n\n[]\n\n\n\nfor p in model.parameters(): print(p.shape)\n\n\nclass Model(nn.Module):\n    def __init__(self, w, b):\n        super().__init__()\n        layers = [Lin(w, b), nn.ReLU()]\n        self.layers = nn.ModuleList(layers)\n        self.loss = nn.MSELoss()        \n    def forward(self, x, targ):\n        for l in self.layers: \n            x = l(x)\n        self.y = x\n        return self.loss(x, targ)\n\n\nmodel = Model(w, b)\n\n\nlist(model.named_children())\n\n[('layers',\n  ModuleList(\n    (0): Lin()\n    (1): ReLU()\n  )),\n ('loss', MSELoss())]\n\n\n\nlist(model.parameters())\n\n[]\n\n\n\nmodel.layers[0].w\n\ntensor([[ 0.3975, -0.1704],\n        [ 0.6908,  0.4523],\n        [ 0.1387,  0.9694],\n        [-0.0032,  1.0253]], requires_grad=True)\n\n\nSee 04_minibatch_training.ipynb to learn how to use parameters and set attrivutes and register modules (sections Using parameters and optim)\n\n\nDesigning models and layers with PyTorch and nn.Sequential()\n\nmodel = nn.Sequential(nn.Linear(M,H), nn.ReLU(), nn.Linear(H,10))\n\n\nmodel\n\nSequential(\n  (0): Linear(in_features=4, out_features=2, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=2, out_features=10, bias=True)\n)\n\n\n\nlayers = [nn.Linear(M,H), nn.ReLU(), nn.Linear(H,10)]\nmodel = nn.Sequential(*layers)\n\n\nmodel\n\nSequential(\n  (0): Linear(in_features=4, out_features=2, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=2, out_features=10, bias=True)\n)\n\n\n\nfor p in model.parameters(): print(p.shape)\n\ntorch.Size([2, 4])\ntorch.Size([2])\ntorch.Size([10, 2])\ntorch.Size([10])\n\n\n\nlist(model.parameters())\n\n[Parameter containing:\n tensor([[-0.2547,  0.3678, -0.3120, -0.4402],\n         [-0.0581,  0.0541, -0.0433,  0.2772]], requires_grad=True),\n Parameter containing:\n tensor([ 0.2642, -0.2418], requires_grad=True),\n Parameter containing:\n tensor([[ 0.1416,  0.5259],\n         [ 0.1009, -0.4274],\n         [-0.2409,  0.4319],\n         [-0.2980,  0.1193],\n         [ 0.3526,  0.3844],\n         [-0.3655, -0.0480],\n         [ 0.2450, -0.4946],\n         [-0.3897,  0.2618],\n         [-0.1028,  0.5648],\n         [ 0.5452, -0.5314]], requires_grad=True),\n Parameter containing:\n tensor([ 0.3573,  0.3235, -0.6281, -0.4056, -0.6562, -0.1154,  0.0053, -0.3871,\n          0.4785, -0.0720], requires_grad=True)]\n\n\n\n# Example of a custom layer\n# Global Average Pooling Layer (Adaptive Average Pooling Layer)\n\nclass GlobalAvgPooling(nn.Module):\n    def forward(self, x):\n        return x.mean((-2, -1))"
  }
]