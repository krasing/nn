[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome to my blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!"
  },
  {
    "objectID": "posts/blog-with-quarto/index.html",
    "href": "posts/blog-with-quarto/index.html",
    "title": "Създаване на блог с quarto",
    "section": "",
    "text": "Създаване на блог за техническа информация с използване на платформата quarto"
  },
  {
    "objectID": "posts/blog-with-quarto/index.html#характеристики",
    "href": "posts/blog-with-quarto/index.html#характеристики",
    "title": "Създаване на блог с quarto",
    "section": "Характеристики:",
    "text": "Характеристики:\n\nпроста и интуитивна предварително зададена структура\nподдържа md синтаксис за въвеждане на текст, заглавия и секции, връзки, картинки, таблици\nподдържа преобразуване на jupyter тетрадки, съдържащи код, формули, графики"
  },
  {
    "objectID": "posts/blog-with-quarto/index.html#основни-стъпки",
    "href": "posts/blog-with-quarto/index.html#основни-стъпки",
    "title": "Създаване на блог с quarto",
    "section": "Основни стъпки:",
    "text": "Основни стъпки:\n\nлокално инсталиране https://quarto.org/docs/get-started/\nгенериране на структурата https://quarto.org/docs/websites/website-blog.html:\n\nотваря се терминал от папката, където ще се съхранява блога и се изпълнява команда:\n quarto create-project myblog --type website:blog\nвизуализира се генерирания блог\n quarto preview myblog\n\nредактират се различните файлове за запознаване със структурата.\n\nЗаглавната страница е във файл index.qmd в коренната директория.\nОбща информация за блога се дава във файл about.qmd в коренната директория\nВсяко съобщение има папка в папка posts и файл index.qmd с основното съдържание. В папката може да има и картинки.\n\nизбира се тема за оформлението на сайта и се задава във файл _quarto.yml:\n\n     format:\n       html:\n         theme: litera\n         css: styles.css\nТемата по подразбиране е cosmo и е със синя заглавна лента. Темата, избрана за този блог е litera с по-убити цветове. Интересна тема е journal, както и много други (cerulean, cyborg, darkly, flatly, lumen, lux, materia, minty, morph, pulse, quartz, sandstone, simplex, sketchy, slate, solar, spacelab, superhero, united, vapor, yeti, zephyr)."
  },
  {
    "objectID": "posts/blog-with-quarto/index.html#публикуване-на-jupyter-тетрадки",
    "href": "posts/blog-with-quarto/index.html#публикуване-на-jupyter-тетрадки",
    "title": "Създаване на блог с quarto",
    "section": "Публикуване на Jupyter тетрадки",
    "text": "Публикуване на Jupyter тетрадки\n\nJust put the notebook in the “posts” folder\nAt the beginning of the jupyter notebook we need to add a cell of type raw with metadata, e.g.\n\n---\ntitle: \"Gradients, broadcasting and backpropagation\"\nauthor: \"me\"\ndate: \"2022-11-20\"\ncategories: [NN]\nformat:\n  html:\n    code-fold: false\njupyter: python3\n---"
  },
  {
    "objectID": "posts/loss/loss.html",
    "href": "posts/loss/loss.html",
    "title": "The Loss",
    "section": "",
    "text": "TO DO: clarify notations (not consistent yet!), add visualizations, add multi-label case\nLet’ define \\(z\\) as the output of the last linear layer (no activation):\n\\(z = [z_1, \\dots, z_h, \\dots, z_H]\\)\nBefore the loss can be calculated some non-linear transformations may be needed. For example, a classification problem requires the output to be interpreted as a probability for the class assigned to the output node. Therefore the output is skewed to fit in the range [0, 1].\nIt should be noted that this need is not limited to the classification tasks. In a regression problem for predicting values with high dynamic ranges, the same error does not have the same effect for all predictions, e.g. prediction error of \\$1 for a product of \\$1000 is small and acceptable, but the same error when the product is \\$2 is not good. The simplest solution in such situation is to use logarithm, but this is out of the scope of this tutorial.\nThe standard loss functions are crossentropy (for classification) and mean squared error (for regression). The non-linearity could be added as a separate layer or could be part of the Loss calculation. This changes the actual output and loss."
  },
  {
    "objectID": "posts/loss/loss.html#logsoftmax-calculation",
    "href": "posts/loss/loss.html#logsoftmax-calculation",
    "title": "The Loss",
    "section": "Log(Softmax) calculation",
    "text": "Log(Softmax) calculation\n\ndef log_softmax(x):\n    '''Logarithm of predicted probabilities calculated from the output'''\n    return softmax(x).log()\n\n\ntest_close(log_softmax(z), torch.log_softmax(z, dim=-1))\n\n\ntest_close(log_softmax(zz), torch.log_softmax(zz, dim=-1))\n\n\ndef log_softmax2(x):\n    return x - x.exp().sum(dim=-1, keepdim=True).log()\n\n\ntest_close(log_softmax2(z), torch.log_softmax(z, dim=-1))\n\n\ntest_close(log_softmax2(zz), torch.log_softmax(zz, dim=-1))\n\n\ndef logsumexp(x):\n    # a = x.max(dim=-1, keepdim=True)[0]\n    # return a + (x-a).exp().sum(dim=-1, keepdim=True).log()\n    a = x.max(dim=-1)[0]\n    return a + (x-a[...,None]).exp().sum(dim=-1).log()\n\n\ntest_close(logsumexp(z), torch.logsumexp(z, dim=-1))\n\n\ntest_close(logsumexp(zz), torch.logsumexp(zz, dim=-1))\n\n\ndef log_softmax3(x):\n    return x - logsumexp(x).unsqueeze(-1)\n\n\ntest_close(log_softmax3(z), torch.log_softmax(z, dim=-1))\n\n\ntest_close(log_softmax3(zz), torch.log_softmax(zz, dim=-1))"
  },
  {
    "objectID": "posts/loss/loss.html#cross-entropy-loss-for-log-probabilities-f.nll_loss",
    "href": "posts/loss/loss.html#cross-entropy-loss-for-log-probabilities-f.nll_loss",
    "title": "The Loss",
    "section": "Cross-entropy loss for log-probabilities, F.nll_loss()",
    "text": "Cross-entropy loss for log-probabilities, F.nll_loss()\n\\(\\mathbb{L} = - \\sum_{i=1}^N \\sum_{h=1}^H{y_{ij} \\ln(p(c_{ih})) } = - \\sum_{i=1}^N {\\ln(\\hat{y}_{i}) }\\)\nIf case of training with such a loss function, the output of the network should be interpreted as log-probabilities. To convert to probabilities, take the exponent of the predictions. My note: a kind of failure intensity: \\(p = \\exp(-\\lambda t) \\implies \\ln(p) = -\\lambda t\\) compare with \\(q = 1 - p = 1 - \\exp(-\\lambda t) \\approx \\lambda t\\).\n\ndef nll(x, y):\n    '''Take the mean value of the correct x\n       x: pred_as_log_softmax\n       y: target_as_index\n    '''\n    N = y.shape[0]\n    loss = -x[range(N), y].mean()\n    return loss\n\n\nimport torch.nn.functional as F\n\n\nvv = -torch.rand((6, 2))*5\ny = torch.randint(0, 2, (6,))\nnll(vv, y)\n\ntensor(3.6464)\n\n\n\n# notice that the log-likelihood values are negative!\nvv, y, [c[yi] for yi in y]\n\n(tensor([[-4.3884, -2.6768],\n         [-2.7044, -4.9118],\n         [-3.0371, -2.6989],\n         [-3.3646, -3.6605],\n         [-4.5365, -1.6881],\n         [-3.0555, -2.3157]]),\n tensor([1, 1, 0, 1, 0, 0]),\n ['female', 'female', 'male', 'female', 'male', 'male'])\n\n\n\ntest_close(nll(vv,y), F.nll_loss(vv, y))\n\n\n# explore the correspondance between log-likelihoods and linelihoods\na = tensor([-4, -3, -2, -1, -0.5, -0.2, -0.05, 0, 0.5])\naa = a.exp()\nprint(['%.2f' %i for i in a])\nprint(['%.3f' %i for i in aa])\n\n['-4.00', '-3.00', '-2.00', '-1.00', '-0.50', '-0.20', '-0.05', '0.00', '0.50']\n['0.018', '0.050', '0.135', '0.368', '0.607', '0.819', '0.951', '1.000', '1.649']"
  },
  {
    "objectID": "posts/loss/loss.html#cross-entropy-loss-for-raw-outputs-f.cross_entropy",
    "href": "posts/loss/loss.html#cross-entropy-loss-for-raw-outputs-f.cross_entropy",
    "title": "The Loss",
    "section": "Cross-entropy loss for raw outputs, F.cross_entropy()",
    "text": "Cross-entropy loss for raw outputs, F.cross_entropy()\nCross-entropy is calculated directly from the output without conversion to probabilities or log-probabilities. All these operations are included in the loss function calculation. The interpretation of the output is unclear, could be any value from \\(-\\infty\\) to \\(\\infty\\) – useful for other regression tasks\n\ndef cross_entropy(x, y):\n    return nll(log_softmax3(x), y)\n\n\ncross_entropy(zz, y)\n\ntensor(1.0843)\n\n\n\ntest_close(cross_entropy(zz, y), F.cross_entropy(zz, y))"
  },
  {
    "objectID": "posts/loss/loss.html#cross-entropy-loss-for-multi-label-target",
    "href": "posts/loss/loss.html#cross-entropy-loss-for-multi-label-target",
    "title": "The Loss",
    "section": "Cross-entropy loss for multi-label target",
    "text": "Cross-entropy loss for multi-label target\n\\(\\mathbb{L} = - \\sum_{i=1}^N \\sum_{h=1}^H{[ y_{ih} \\ln(\\hat{y_{ih}}) + (1 -y_{ih}) \\ln(1 -\\hat{y}_{ih})]}\\)\n\\(\\mathbb{L} = - \\sum_{i=1}^N \\sum_{h=1}^H{[ y_{ih} \\ln(p(c_{ih})) + (1 -y_{ih}) \\ln(1 -p(c_{ih}))]}\\)\n\\(\\mathbb{L} = - \\sum_{i=1}^N \\sum_{h=1}^H \\sum_{k=1}^2{y_{ihk} \\ln(p(c_{ih}[k])) }\\)\n\\(\\mathbb{L} = - \\sum_{i=1}^N{[ y_i \\ln(\\mathrm{sigmoid(z_i)}) + (1 -y_i) \\ln(1 -\\mathrm{sigmoid(z_i)})]}\\)\n\n# Generate test targets - a kind of one hot encoded but multiple ones are allowed\nyy = torch.zeros_like(zz)\nyy[range(len(yy)), y] = 1\nprint('Target as index: ', y)\nprint('The same, one hot encoding:\\n', yy)\nyy[0, 0] = 1\nprint('Both ones for the first sample:\\n', yy)\n\nTarget as index:  tensor([1, 1, 0, 1, 0, 0])\nThe same, one hot encoding:\n tensor([[0., 1.],\n        [0., 1.],\n        [1., 0.],\n        [0., 1.],\n        [1., 0.],\n        [1., 0.]])\nBoth ones for the first sample:\n tensor([[1., 1.],\n        [0., 1.],\n        [1., 0.],\n        [0., 1.],\n        [1., 0.],\n        [1., 0.]])\n\n\n\ndef cross_entropy_multi(z, y):\n    '''z: pred as unnormalized scores\n       y: target as binary encoded aray'''\n    s = torch.sigmoid(z)\n    loss = -(y * s.log() + (1 - y)*(1 - s).log()).mean()\n    return loss\n\n\ncross_entropy_multi(zz, yy)\n\ntensor(1.1704)\n\n\n\nF.binary_cross_entropy_with_logits(zz, yy)\n\ntensor(1.1704)\n\n\n\nF.binary_cross_entropy(torch.sigmoid(zz), yy)\n\ntensor(1.1704)"
  },
  {
    "objectID": "posts/gradients/gradients.html",
    "href": "posts/gradients/gradients.html",
    "title": "Gradients, broadcasting and backpropagation",
    "section": "",
    "text": "Let’s have \\(y = w \\cdot x + b\\). Application of the rules of differentiation is simple, e.g.\n\\[\\frac{\\mathrm{d}y}{\\mathrm{d}x} = w , \\quad\n\\frac{\\mathrm{d}y}{\\mathrm{d}w} = x, \\quad\n\\frac{\\mathrm{d}y}{\\mathrm{d}b} = 1\\]\nThe change in \\(y\\) is porportional to the change in \\(x\\). The bigger is \\(w\\), the bigger is the change of \\(y\\) for the same change of \\(x\\).\nLet’s \\(L = f(y) = f(y(x))\\). Application of the chain rule is simple, e.g.\n\\[\\frac{\\mathrm{d}L}{\\mathrm{d}x} = \\frac{\\mathrm{d}f(y)}{\\mathrm{d}y} \\cdot \\frac{\\mathrm{d}y}{\\mathrm{d}x} = \\frac{\\mathrm{d}L}{\\mathrm{d}y} \\cdot w\\] \\[\\frac{\\mathrm{d}L}{\\mathrm{d}w} = \\frac{\\mathrm{d}f(y)}{\\mathrm{d}y} \\cdot \\frac{\\mathrm{d}y}{\\mathrm{d}w} = \\frac{\\mathrm{d}L}{\\mathrm{d}y} \\cdot x\\] \\[\\frac{\\mathrm{d}L}{\\mathrm{d}b} = \\frac{\\mathrm{d}f(y)}{\\mathrm{d}y} \\cdot \\frac{\\mathrm{d}y}{\\mathrm{d}b} = \\frac{\\mathrm{d}L}{\\mathrm{d}y} \\cdot 1\\]\nThe multidimensional case is not so simple. Functions with multiple inputs and multiple outputs have multiple partial derivatives which need to be arranged and stored properly. Applying this for batches of data complicates the picture even more."
  },
  {
    "objectID": "posts/gradients/gradients.html#derivative-of-a-transformation",
    "href": "posts/gradients/gradients.html#derivative-of-a-transformation",
    "title": "Gradients, broadcasting and backpropagation",
    "section": "Derivative of a transformation",
    "text": "Derivative of a transformation\nThe derivative of a function (transformation \\(\\psi\\)) with multiple inputs \\(\\mathbf{x} \\in \\mathbb{R}^M\\) and multiple outputs \\(\\mathbf{y} \\in \\mathbb{R}^H\\) is a matrix containing the partial derivatives of each output with respect to each input (the so called Jacobian of the transformation, \\(\\frac{\\partial{\\mathbf{y}}}{\\partial{\\mathbf{x}}} \\in \\mathbb{R}^{M \\times H}\\)). For example, if \\(M=4\\) and \\(H=2\\) we can write:\n\\(\\mathbf{y} = \\psi(\\mathbf{x})\\),\n\\(\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4\\end{bmatrix}, \\quad \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix}\\)\n\\(\\frac{\\partial{\\mathbf{y}}}{\\partial{\\mathbf{x}}} = \\begin{bmatrix} \\frac{\\partial{y_1}}{\\partial{x_1}} & \\frac{\\partial{y_1}}{\\partial{x_2}} & \\frac{\\partial{y_1}}{\\partial{x_3}} & \\frac{\\partial{y_1}}{\\partial{x_4}}\\\\ \\frac{\\partial{y_2}}{\\partial{x_1}} & \\frac{\\partial{y_2}}{\\partial{x_2}} & \\frac{\\partial{y_2}}{\\partial{x_3}} & \\frac{\\partial{y_2}}{\\partial{x_4}} \\end{bmatrix}\\)\nThe derivative of \\(\\mathbf{y = W \\cdot x}\\) with respect to \\(\\mathbf{x}\\) is \\(\\mathbf{W}\\), i.e. \\(\\frac{∂y}{∂x} = W\\).\nWe should note that in neural networks the input and output features are arranged as raw vectors.\nThe derivative of \\(\\mathbf{y = x \\cdot W}\\) with respect to \\(\\mathbf{x}\\) is \\(\\mathbf{W^T}\\), i.e. \n\\[\\frac{∂y}{∂x} = W^T\\]"
  },
  {
    "objectID": "posts/gradients/gradients.html#the-chain-rule",
    "href": "posts/gradients/gradients.html#the-chain-rule",
    "title": "Gradients, broadcasting and backpropagation",
    "section": "The chain rule",
    "text": "The chain rule\nLet’s have \\(y = x \\cdot W + b\\) and \\(L = f(y)\\)\n\n\n\n\n\n\nSimple NN\n\n\n\n\n\n\n\nMatrix view, xW\n\n\n\n\n\nThe chain rule involves propagating the gradient of the loss layer after layer backward towards the inputs and parameters of interest. In our demonstration case in order to calculate the gradient of \\(L\\) with respect to the input \\(x\\) we need to have the gradient of \\(L\\) with respect to the output \\(y\\).\nThe shapes of the gradient is the same as the shape of the corresponding variable (parameter)\nA gradient is attached to each variable and parameter of the model, i.e.\n\\(y.g = \\frac{\\partial{L}}{∂{y}}\\)\n\\(x.g = \\frac{\\partial{L}}{∂{x}} = \\frac{\\partial{L}}{\\partial{y}} \\cdot \\frac{\\partial{y}}{\\partial{x}} = y.g \\cdot \\frac{\\partial{y}}{\\partial{x}} = y.g \\cdot W^T\\)\n\\(b.g = \\frac{\\partial{L}}{∂{b}} = \\frac{\\partial{L}}{\\partial{y}} \\cdot \\frac{\\partial{y}}{\\partial{b}} = y.g \\cdot \\frac{\\partial{y}}{\\partial{b}} = y.g\\)\n\\(W.g = \\frac{\\partial{L}}{∂W} = ((\\frac{\\partial{L}}{\\partial{y}})^T \\cdot \\frac{\\partial{y}}{\\partial{W}})^T = (y.g^T \\cdot x)^T = x^T \\cdot y.g\\)\n \nNotes:\n\n\\(x\\), \\(y\\) and \\(b\\) are row vectors.\n\\(W\\) is a weight matrix with \\(m\\) rows and \\(h\\) columns;\n\\(x\\) includes the \\(m\\) input features\n\\(b\\) is a bias with \\(h\\) elements;\n\\(y\\) has \\(h\\) features (or nodes).\n\\(x\\) and \\(y\\) represent input and output features (variables, nodes in the NN). Adding additional dimension (multiple rows) could represent multiple data samples. Inputs and outputs could be replaced by matrices \\(X\\) and \\(Y\\) where the last dimension gives the features (\\(x\\) and \\(y\\) for the corresponding data point);"
  },
  {
    "objectID": "posts/gradients/gradients.html#implementation-and-testing-with-code",
    "href": "posts/gradients/gradients.html#implementation-and-testing-with-code",
    "title": "Gradients, broadcasting and backpropagation",
    "section": "Implementation and testing with code",
    "text": "Implementation and testing with code\nAbove equations are implemented in function lin_grad and tested in the following code\n\ndef lin_grad(x, w, b, y):\n    # y.g shoudl be available!\n    b.g = y.g.sum(dim=0)#/y.shape[0]\n    w.g = x.T @ y.g\n    x.g = y.g @ w.T\n\n\nimport torch\nfrom torch import tensor\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n\n# Generate test input and ouput data\nN = 100 # Number of samples\nM = 4 # Number of input features\nH = 2 # Number of outputs\nx = torch.rand((N, M))*10 - 5 # Input\nk1, k2, k3, k4, k6, k7 = 1, 1.5, 2, 2.5, 3.0, -0.5\nW_true = torch.tensor([[k1, k1],\n                  [k2, k2],\n                  [k3, k6],\n                  [k4, k7]])\nb_true = torch.tensor([0, -1])\ny = x @ W_true + b_true # Output\nW_true, y[:7]\n\n(tensor([[ 1.0000,  1.0000],\n         [ 1.5000,  1.5000],\n         [ 2.0000,  3.0000],\n         [ 2.5000, -0.5000]]),\n tensor([[-11.5291, -12.7022],\n         [ -3.9763, -22.8349],\n         [  2.2930,   7.2452],\n         [ -4.9708,   6.3649],\n         [-11.7199, -24.9706],\n         [ 14.8387,   5.4946],\n         [ -9.7971,  -0.4925]]))\n\n\n\nplt.scatter(x[:,2], y[:,1]);\nplt.xlabel('Input, dimension (feature) No3');\nplt.ylabel('Output, dim. No2');\n\n\n\n\n\n# Generate random weights\nw = torch.randn(M,H)\nb = torch.zeros(H)\nw, b\n\n(tensor([[-0.0383, -0.8173],\n         [ 0.8458, -2.0662],\n         [ 0.1153,  0.0775],\n         [ 1.0845, -0.1016]]),\n tensor([0., 0.]))\n\n\nIn order to test the function lin_grad we need to calculate the gradient \\(∂L/dy\\) and save it in y_pred.g (the gradient with respect to the prediction):\n\ndefine model and calculate prediction\ndefine and calculate loss \\(L\\) as simple mean squared error mse(y_pred, y_targ)\ndefine and run mse_grad(y_pred, y_targ)\nrun lin_grad(x, w, b, y)\n\n\ndef lin(x, w, b): return x @ w + b\n\n\ny_pred = lin(x, w, b)\ny_pred[:5]\n\ntensor([[ 0.0472, -0.5698],\n        [ 1.4009,  5.7816],\n        [-3.4003,  1.4367],\n        [-0.9559, -5.0894],\n        [-0.9125,  9.5171]])\n\n\n\ndef mse(y_pred, y_targ): return (y_pred-y_targ).pow(2).mean()\n\n\nloss = mse(y_pred, y)\nloss\n\ntensor(181.8108)\n\n\n\ndef mse_grad(y_pred, y_targ): y_pred.g = 2 * (y_pred - y_targ) / y_targ.shape[0] / y_targ.shape[1]\n\n\nmse_grad(y_pred, y)\n\nFinnally, test if all dimensions in lin_grad match:\n\nlin_grad(x, w, b, y_pred)\n\nNext test if the loss improves\n\nw -= 0.1 * w.g\nb -= 0.1 * b.g\ny_pred = lin(x, w, b)\nloss = mse(y_pred, y)\nloss\n\ntensor(9.2100)\n\n\n\nfor i in range(20):\n    y_pred = lin(x, w, b)\n    loss = mse(y_pred, y)\n    mse_grad(y_pred, y)\n    lin_grad(x, w, b, y_pred)\n    w -= 0.01 * w.g\n    b -= 0.01 * b.g\n    print(loss, end=', ')\nprint('End!\\n')\nprint('Weights and biases of the network and for the dataset:')\nw, b, W_true, b_true\n\ntensor(9.2100), tensor(7.3055), tensor(5.8198), tensor(4.6593), tensor(3.7515), tensor(3.0402), tensor(2.4818), tensor(2.0425), tensor(1.6961), tensor(1.4221), tensor(1.2048), tensor(1.0318), tensor(0.8936), tensor(0.7827), tensor(0.6933), tensor(0.6208), tensor(0.5616), tensor(0.5130), tensor(0.4729), tensor(0.4394), End!\n\nWeights and biases of the network and for the dataset:\n\n\n(tensor([[ 1.0095,  0.9741],\n         [ 1.5336,  1.5728],\n         [ 1.9631,  3.0287],\n         [ 2.3814, -0.5464]]),\n tensor([ 0.0677, -0.2069]),\n tensor([[ 1.0000,  1.0000],\n         [ 1.5000,  1.5000],\n         [ 2.0000,  3.0000],\n         [ 2.5000, -0.5000]]),\n tensor([ 0, -1]))"
  },
  {
    "objectID": "posts/gradients/gradients.html#pytorch-autograd-and-backpropagation",
    "href": "posts/gradients/gradients.html#pytorch-autograd-and-backpropagation",
    "title": "Gradients, broadcasting and backpropagation",
    "section": "PyTorch autograd and backpropagation",
    "text": "PyTorch autograd and backpropagation\nWe will use PyTorch automatic gradient calculation to check our algorithms. This involves using the build-in methods and parameters .backward() and .grad.\nIn order to apply PyTorch backpropagation and autograd we need to define a forward function that relates the inputs with the loss:\n\ndef forward(x, y):\n    y_pred = lin(x, w, b)\n    loss =  mse(y_pred, y)\n    return loss\n\n\nloss = forward(x, y)\nloss\n\ntensor(0.4113)\n\n\nBut this is not enough:\n    loss.backward()\n–> RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\nLooks like a slot for saving the gradient to the corresponding inputs and model parameters should be required.\n\n# Update gradients as above - manual backward - for comparison\n\n# forward\ny_pred = lin(x, w, b)\nloss =  mse(y_pred, y)\n\n# backward\nmse_grad(y_pred, y)\nlin_grad(x, w, b, y_pred)\n\n# Good to know: the parameters are not updated in the backward pass!\n\n\n# Require gradient to keep them with the data\n\nfor element in [x, w, b]:\n    element.requires_grad_(True)\n\n\nloss = forward(x, y)\nloss.backward()\n\n\nloss\n\ntensor(0.4113, grad_fn=<MeanBackward0>)\n\n\n\nfrom fastcore.test import test_close\n\n\ntest_close(x.g, x.grad)\ntest_close(w.g, w.grad)\ntest_close(b.g, b.grad)"
  },
  {
    "objectID": "posts/gradients/gradients.html#what-is-next",
    "href": "posts/gradients/gradients.html#what-is-next",
    "title": "Gradients, broadcasting and backpropagation",
    "section": "What is next?",
    "text": "What is next?\nThe next step is to try to create a “proper” non-linear neural network.\nThe structure of a fully connected neural network with single hidden layer could be represented as follows:\n\n\n\nNN with one hidden layer\n\n\nA few more gradients need to be defined and calculated"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My notes and clarifications for the fast.ai course 2022p2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Neural Networks",
    "section": "",
    "text": "The Loss\n\n\n\n\n\n\n\nNN\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2022\n\n\nKrasin Georgiev\n\n\n\n\n\n\n  \n\n\n\n\nGradients, broadcasting and backpropagation\n\n\n\n\n\n\n\nNN\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\nKrasin Georgiev\n\n\n\n\n\n\n  \n\n\n\n\nСъздаване на блог с quarto\n\n\n\n\n\n\n\nwebtools\n\n\nbg\n\n\n\n\n\n\n\n\n\n\n\nNov 18, 2022\n\n\nme\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to my blog\n\n\n\n\n\n\n\nadmin\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\n\n\n\n\nNo matching items"
  }
]