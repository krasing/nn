{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Hiding the details - the Module class\"\n",
    "author: \"Krasin Georgiev\"\n",
    "date: \"2023-01-03\"\n",
    "image: \"custom_layer.png\"\n",
    "categories: [NN, code]\n",
    "format:\n",
    "  html:\n",
    "    code-fold: false\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to build a NN model? When you drive a car you don't see all car and engine parameters. Even when an engineer designs a car he don't know  details about each component (mechanical, electrical, electro-mechanical or electronic part). This is done to manage the complexity. This approach is especially important for software design where the number of components is much greater and design flexibility is colse to infinity.\n",
    "\n",
    "The code was adapted from [fastai course 2022, part 2](https://github.com/fastai/course22p2) while studying notebooks `03_backprop.ipynb` and `04_minibatch_training.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers\n",
    "\n",
    "Some of the most widely used layers in a NN are Linear, ReLU and Mse.\n",
    "\n",
    "The weights of the layer are initiated when the object is created and are defined in the `__init__` method. The relationship for calculating the output from the input are defined in the `__call__` layer (the \"forward\" feature). The functionality for calculation of the gradients is provided in the `backward` method. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin():\n",
    "    def __init__(self, w, b):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "    def __call__(self, x):\n",
    "        y = x @ self.w + self.b\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return self.y\n",
    "    def backward(self):        \n",
    "        self.b.g = self.y.g.sum(dim=0)\n",
    "        self.w.g = self.x.T @ self.y.g\n",
    "        self.x.g =  self.y.g @ self.w.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    def __call__(self, x):\n",
    "        self.x = x\n",
    "        self.y = self.relu(x)\n",
    "        return self.y\n",
    "    def relu(self, x): return x.clamp_min(0.)\n",
    "    def backward(self): self.x.g = self.y.g * (self.x > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse():\n",
    "    def __call__(self, pred, y):\n",
    "        self.pred = pred\n",
    "        self.targ = y\n",
    "        return (pred-y).pow(2).mean()\n",
    "    def backward(self):\n",
    "        self.pred.g = 2 * (self.pred - self.targ) / self.targ.shape[0] / self.targ.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code demonstrates the creation and use of a linear layer object. Look for:\n",
    "\n",
    "- Initialization: `lin = Lin(w, b`\n",
    "\n",
    "- Forward application: `y = lin(x)`\n",
    "\n",
    "- Backpropagation: `lin.backward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from fastcore.test import test_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = \n",
      " tensor([[ 3.6329, -1.5028,  1.5645, -2.5170],\n",
      "        [ 4.4755,  3.9404, -3.6032, -0.0517],\n",
      "        [ 1.0234, -3.7332,  3.1473,  0.6829]]) \n",
      "y = \n",
      " tensor([[ 0.6308, -2.3627],\n",
      "        [ 4.0016, -2.5262],\n",
      "        [-1.7378,  1.8884]])\n"
     ]
    }
   ],
   "source": [
    "# Generate random weights for the model\n",
    "M = 4 # Number of input features\n",
    "H = 2 # Number of outputs\n",
    "w = torch.randn(M,H)\n",
    "b = torch.zeros(H)\n",
    "\n",
    "# Create linear layer object\n",
    "lin = Lin(w, b)\n",
    "\n",
    "# Simulate inputs\n",
    "N = 3 # Number of samples\n",
    "x = torch.rand((N, M))*10 - 5 # just random numbers as example\n",
    "\n",
    "# Calculate the output of the layer\n",
    "y = lin(x)\n",
    "\n",
    "# Show inputs and outputs\n",
    "print('x = \\n', x, '\\ny = \\n', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.g = \n",
      " tensor([[0.2319, 0.9382, 0.8166, 0.7303],\n",
      "        [0.2856, 0.8208, 0.5457, 0.4417],\n",
      "        [0.2376, 0.8698, 0.7110, 0.6232]]) \n",
      "w.g = \n",
      " tensor([[ 8.1594,  5.1629],\n",
      "        [-0.9825, -1.6451],\n",
      "        [ 0.8388,  1.4778],\n",
      "        [-1.6999, -1.4052]]) \n",
      "b.g = \n",
      " tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Provide output gradients (usually based on the Loss: y.g = dL/dy)\n",
    "y.g = torch.rand(y.shape) # output gradients are needed in order for backpropagation to work\n",
    "\n",
    "# Calculate (backpropagate) gradients\n",
    "lin.backward()\n",
    "\n",
    "# Show gradients\n",
    "print('x.g = \\n', x.g, '\\nw.g = \\n', w.g, '\\nb.g = \\n', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3.6329, -1.5028,  1.5645, -2.5170],\n",
       "         [ 4.4755,  3.9404, -3.6032, -0.0517],\n",
       "         [ 1.0234, -3.7332,  3.1473,  0.6829]]),\n",
       " tensor([[3.6329, 0.0000, 1.5645, 0.0000],\n",
       "         [4.4755, 3.9404, 0.0000, 0.0000],\n",
       "         [1.0234, 0.0000, 3.1473, 0.6829]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu = ReLU()\n",
    "y_relu = relu(x)\n",
    "x, y_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_relu.g = torch.rand(y_relu.shape)\n",
    "relu.backward()\n",
    "x_relu_g = x.g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model is an arrangement of one or more layers. It contains all the weights and relationships that allow an input to be transformed into an output and loss and the derivative of the loss to be back-propagated to the model weights and inputs. Each layer can be considered a simple model. Each model have the same general methods as a layer.\n",
    "\n",
    "A kind of a distinction between a model and layer could be introduced if the explicit output of the model is restricted to be a scalar - the loss. But the loss is quite often calculated separately over the outputs of the model in which case the models are just more complex layers.\n",
    "\n",
    "The simplest arrangement is a sequence of layers where the output of each layer (except the last one) is input to the next layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A model with just a Linear, ReLU and Loss layers\n",
    "\n",
    "# The output of the model is the loss. \n",
    "# In additon, the output of the last layer (usually the loss is not counted as a layer)\n",
    "# is saved as model attribure self.y\n",
    "\n",
    "class Model():\n",
    "    def __init__(self, w, b):\n",
    "        self.layers = [Lin(w, b), ReLU()]\n",
    "        self.loss = Mse()\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers: \n",
    "            x = l(x)\n",
    "        self.y = x\n",
    "        return self.loss(x, targ)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "        for l in reversed(self.layers): \n",
    "            l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate target values (needed to calculate the loss)\n",
    "k1, k2, k3, k4, k6, k7 = 1, 1.5, 2, 2.5, 3.0, -0.5\n",
    "W_true = torch.tensor([[k1, k1],\n",
    "                  [k2, k2],\n",
    "                  [k3, k6],\n",
    "                  [k4, k7]])\n",
    "b_true = torch.tensor([0, -1])\n",
    "y_target = x @ W_true + b_true # Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.5302)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = model(x, y_target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6308, 0.0000],\n",
       "        [4.0016, 0.0000],\n",
       "        [0.0000, 1.8884]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model.y\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.g = \n",
      " tensor([[ 0.3201,  0.5563,  0.1117, -0.0025],\n",
      "        [ 0.1260,  0.2190,  0.0440, -0.0010],\n",
      "        [ 0.0929, -0.2466, -0.5286, -0.5591]]) \n",
      "w.g = \n",
      " tensor([[ 4.3441, -0.5581],\n",
      "        [ 0.0389,  2.0357],\n",
      "        [ 0.1176, -1.7162],\n",
      "        [-2.0432, -0.3724]]) \n",
      "b.g = \n",
      " tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Show gradients\n",
    "print('x.g = \\n', x.g, '\\nw.g = \\n', w.g, '\\nb.g = \\n', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Module class and class inheritance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above classes can be based on a more general class so and more information can be hidden, e.g. saving parameter gradients, saving inputs and outputs, etc. Only the initialization and the functions needed to do the forward and backward pass need to be redefined. All we have is modules and submodules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module():        \n",
    "    def __call__(self, *x):\n",
    "        self.x = x\n",
    "        self.y = self.forward(*x)\n",
    "        return self.y    \n",
    "    def backward(self):\n",
    "        self.bwd(self.y, *self.x)\n",
    "    def forward(self):\n",
    "        raise Exception('Not implemented')    \n",
    "    def bwd(self):\n",
    "        raise Exception('Not implemented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin(Module):\n",
    "    def __init__(self, w, b):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "    def forward(self, x):\n",
    "        y = x @ self.w + self.b\n",
    "        return y\n",
    "    def bwd(self, y, x):        \n",
    "        self.b.g = y.g.sum(dim=0)\n",
    "        self.w.g = x.T @ y.g\n",
    "        x.g =  y.g @ self.w.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear layer object\n",
    "lin = Lin(w, b)\n",
    "\n",
    "# Calculate the output of the layer\n",
    "y2 = lin(x)\n",
    "\n",
    "# Test results\n",
    "test_close(y, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin.y.g = torch.rand(lin.y.shape)\n",
    "lin.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(x.g, lin.x[0].g)\n",
    "test_close(w.g, lin.w.g)\n",
    "test_close(b.g, lin.b.g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def forward(self, x): return x.clamp_min(0.)\n",
    "    def bwd(self, y, x): x.g = y.g * (x > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = ReLU()\n",
    "test_close( relu(x), y_relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu.y.g = y_relu.g\n",
    "relu.backward()\n",
    "test_close(relu.x[0].g, x_relu_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse(Module):\n",
    "    def forward(self, pred, y): return (pred-y).pow(2).mean()\n",
    "    def bwd(self, out, pred, targ):\n",
    "        pred.g = 2 * (pred - targ) / targ.shape[0] / targ.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(model(x, y_target), loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.g = \n",
      " tensor([[ 0.3201,  0.5563,  0.1117, -0.0025],\n",
      "        [ 0.1260,  0.2190,  0.0440, -0.0010],\n",
      "        [ 0.0929, -0.2466, -0.5286, -0.5591]]) \n",
      "w.g = \n",
      " tensor([[ 4.3441, -0.5581],\n",
      "        [ 0.0389,  2.0357],\n",
      "        [ 0.1176, -1.7162],\n",
      "        [-2.0432, -0.3724]]) \n",
      "b.g = \n",
      " tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Show gradients\n",
    "print('x.g = \\n', x.g, '\\nw.g = \\n', w.g, '\\nb.g = \\n', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch layers and Module class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above Module will be replaced by the standard PyTorch class `nn.Module`. The autograd and backpropagation features of PyTorch will be used to remove the need for defining the `bwd` or the `backward`methods. The `Lin` layer will be redefined to inherit `nn.Module`. The `ReLU()` and `Mse()` layers will be replaced by `nn.ReLU()` and `nn.MSELoss()` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin(nn.Module):\n",
    "    def __init__(self, w, b):\n",
    "        super().__init__()\n",
    "        self.w = w.requires_grad_(True)\n",
    "        self.b = b.requires_grad_(True)\n",
    "    def forward(self, x):\n",
    "        y = x @ self.w + self.b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear layer object\n",
    "lin = Lin(w, b)\n",
    "\n",
    "# Calculate the output of the layer\n",
    "y2 = lin(x)\n",
    "\n",
    "# Test results\n",
    "test_close(y, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2.grad = torch.rand(y.shape) # output gradients are needed in order for backpropagation to work\n",
    "\n",
    "# Calculate (backpropagate) gradients\n",
    "# y2.backward()\n",
    "\n",
    "# RuntimeError: grad can be implicitly created only for scalar outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, w, b):\n",
    "        super().__init__()\n",
    "        self.layers = [Lin(w, b), nn.ReLU()]\n",
    "        self.loss = nn.MSELoss()        \n",
    "    def forward(self, x, targ):\n",
    "        for l in self.layers: \n",
    "            x = l(x)\n",
    "        self.y = x\n",
    "        return self.loss(x, targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(x, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.5302, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(w.grad, w.g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(b.grad, b.g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad\n",
    "# None because `x.requires_grad_(True)` was never called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.6329, -1.5028,  1.5645, -2.5170],\n",
       "        [ 4.4755,  3.9404, -3.6032, -0.0517],\n",
       "        [ 1.0234, -3.7332,  3.1473,  0.6829]], requires_grad=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gradients accumulate\n",
    "w.grad.zero_()\n",
    "b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(x, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3201,  0.5563,  0.1117, -0.0025],\n",
       "        [ 0.1260,  0.2190,  0.0440, -0.0010],\n",
       "        [ 0.0929, -0.2466, -0.5286, -0.5591]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(x.grad, x.g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(w.grad, w.g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some more details in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (loss): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# notice that the layers of the model are not properly registered and accessible\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.6329, 0.0000, 1.5645, 0.0000],\n",
       "        [4.4755, 3.9404, 0.0000, 0.0000],\n",
       "        [1.0234, 0.0000, 3.1473, 0.6829]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loss', MSELoss())]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.named_children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.parameters(): print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, w, b):\n",
    "        super().__init__()\n",
    "        layers = [Lin(w, b), nn.ReLU()]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.loss = nn.MSELoss()        \n",
    "    def forward(self, x, targ):\n",
    "        for l in self.layers: \n",
    "            x = l(x)\n",
    "        self.y = x\n",
    "        return self.loss(x, targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('layers',\n",
       "  ModuleList(\n",
       "    (0): Lin()\n",
       "    (1): ReLU()\n",
       "  )),\n",
       " ('loss', MSELoss())]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.named_children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3975, -0.1704],\n",
       "        [ 0.6908,  0.4523],\n",
       "        [ 0.1387,  0.9694],\n",
       "        [-0.0032,  1.0253]], requires_grad=True)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [04_minibatch_training.ipynb](https://github.com/fastai/course22p2/blob/master/nbs/04_minibatch_training.ipynb) to learn how to use parameters and set attrivutes and register modules (sections Using parameters and optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing models and layers with PyTorch and nn.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(M,H), nn.ReLU(), nn.Linear(H,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=2, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=2, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [nn.Linear(M,H), nn.ReLU(), nn.Linear(H,10)]\n",
    "model = nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=2, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=2, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "torch.Size([2])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters(): print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.2547,  0.3678, -0.3120, -0.4402],\n",
       "         [-0.0581,  0.0541, -0.0433,  0.2772]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.2642, -0.2418], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.1416,  0.5259],\n",
       "         [ 0.1009, -0.4274],\n",
       "         [-0.2409,  0.4319],\n",
       "         [-0.2980,  0.1193],\n",
       "         [ 0.3526,  0.3844],\n",
       "         [-0.3655, -0.0480],\n",
       "         [ 0.2450, -0.4946],\n",
       "         [-0.3897,  0.2618],\n",
       "         [-0.1028,  0.5648],\n",
       "         [ 0.5452, -0.5314]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.3573,  0.3235, -0.6281, -0.4056, -0.6562, -0.1154,  0.0053, -0.3871,\n",
       "          0.4785, -0.0720], requires_grad=True)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a custom layer\n",
    "# Global Average Pooling Layer (Adaptive Average Pooling Layer)\n",
    "\n",
    "class GlobalAvgPooling(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.mean((-2, -1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
