{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Gradients, broadcasting and backpropagation\"\n",
    "author: \"Krasin Georgiev\"\n",
    "date: \"2022-11-23\"\n",
    "categories: [NN]\n",
    "format:\n",
    "  html:\n",
    "    code-fold: false\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have $y = w \\cdot x  + b$. Application of the rules of differentiation is simple, e.g. \n",
    "\n",
    "$$\\frac{\\mathrm{d}y}{\\mathrm{d}x} = w , \\quad\n",
    "\\frac{\\mathrm{d}y}{\\mathrm{d}w} = x, \\quad\n",
    "\\frac{\\mathrm{d}y}{\\mathrm{d}b} = 1$$\n",
    "\n",
    "The change in $y$ is porportional to the change in $x$. The bigger is $w$, the bigger is the change of $y$ for the same change of $x$.\n",
    "\n",
    "Let's $L = f(y) = f(y(x))$. Application of the chain rule is simple, e.g. \n",
    "\n",
    "$$\\frac{\\mathrm{d}L}{\\mathrm{d}x} = \\frac{\\mathrm{d}f(y)}{\\mathrm{d}y} \\cdot \\frac{\\mathrm{d}y}{\\mathrm{d}x} = \\frac{\\mathrm{d}L}{\\mathrm{d}y} \\cdot w$$ \n",
    "$$\\frac{\\mathrm{d}L}{\\mathrm{d}w} = \\frac{\\mathrm{d}f(y)}{\\mathrm{d}y} \\cdot \\frac{\\mathrm{d}y}{\\mathrm{d}w} = \\frac{\\mathrm{d}L}{\\mathrm{d}y} \\cdot x$$ \n",
    "$$\\frac{\\mathrm{d}L}{\\mathrm{d}b} = \\frac{\\mathrm{d}f(y)}{\\mathrm{d}y} \\cdot \\frac{\\mathrm{d}y}{\\mathrm{d}b} = \\frac{\\mathrm{d}L}{\\mathrm{d}y} \\cdot 1$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multidimensional case is not so simple. Functions with multiple inputs and multiple outputs have multiple partial derivatives which need to be arranged and stored properly. Applying this for batches of data complicates the picture even more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of a function (transformation $\\psi$) with multiple inputs $\\mathbf{x} \\in \\mathbb{R}^M$ and multiple outputs $\\mathbf{y} \\in \\mathbb{R}^H$ is a matrix containing the partial derivatives of each output with respect to each input (the so called Jacobian of the transformation, $\\frac{\\partial{\\mathbf{y}}}{\\partial{\\mathbf{x}}} \\in \\mathbb{R}^{M \\times H}$). For example, if $M=4$ and $H=2$ we can write:\n",
    "\n",
    "$\\mathbf{y} = \\psi(\\mathbf{x})$,\n",
    "\n",
    "$\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4\\end{bmatrix}, \\quad\n",
    "\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix}$\n",
    "\n",
    "$\\frac{\\partial{\\mathbf{y}}}{\\partial{\\mathbf{x}}} = \\begin{bmatrix} \\frac{\\partial{y_1}}{\\partial{x_1}} & \\frac{\\partial{y_1}}{\\partial{x_2}} & \\frac{\\partial{y_1}}{\\partial{x_3}}  & \\frac{\\partial{y_1}}{\\partial{x_4}}\\\\\n",
    "\\frac{\\partial{y_2}}{\\partial{x_1}} & \\frac{\\partial{y_2}}{\\partial{x_2}} & \\frac{\\partial{y_2}}{\\partial{x_3}} & \\frac{\\partial{y_2}}{\\partial{x_4}}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "We should note that in neural networks the input and output features are arranged as raw vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have $y = x \\cdot W + b$ and $L = f(y)$\n",
    "\n",
    "where \n",
    "\n",
    "- $y$, $x$ and $b$ are row vectors and $W$ is a matrix.\n",
    "- $x$ includes the $m$ input features\n",
    "- $W$ is a weight matrix with $m$ rows and $h$ columns; \n",
    "- $b$ is a bias with $h$ elements; \n",
    "- $y$ has $h$ features (or nodes).\n",
    "- $x$ and $y$ represent input and output features (variables, nodes in the NN). Adding additional dimension (multiple rows) could represent multiple data samples. Inputs and outputs could be replaced by matrices $X$ and $Y$ where the last dimension gives the features ($x$ and $y$ for the corresponding data point); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Simple NN](../nn-mini.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of $\\mathbf{y = A \\cdot x}$ with respect to $\\mathbf{x}$ is $\\mathbf{A}$. \n",
    "\n",
    "The derivative of $\\mathbf{y = x \\cdot A}$ with respect to $\\mathbf{x}$ is $\\mathbf{A^T}$.\n",
    "\n",
    "The chain rule involves matrix multiplication of Jacobians and vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A gradient is attached to each variable and parameter of the model**, i.e.\n",
    "\n",
    "$y.g = \\frac{\\partial{L}}{∂{y}}$\n",
    "\n",
    "$x.g = \\frac{\\partial{L}}{∂{x}} = \\frac{\\partial{L}}{\\partial{y}} \\cdot \\frac{\\partial{y}}{\\partial{x}} = y.g \\cdot \\frac{\\partial{y}}{\\partial{x}} = y.g \\cdot W^T$\n",
    "\n",
    "$b.g = \\frac{\\partial{L}}{∂{b}} = \\frac{\\partial{L}}{\\partial{y}} \\cdot \\frac{\\partial{y}}{\\partial{b}} = y.g \\cdot \\frac{\\partial{y}}{\\partial{b}} = y.g$\n",
    "\n",
    "$W.g = \\partial{L}/∂{W} = ((\\frac{\\partial{L}}{\\partial{y}})^T \\cdot \\frac{\\partial{y}}{\\partial{W}})^T = (y.g^T \\cdot x)^T = x^T \\cdot y.g$\n",
    "\n",
    "The shapes of the gradient is the same as the shape of the corresponding variable (parameter), e.g. `x.g.shape ≡  x.shape`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Output y calculation](../xW.png) \n",
    "![x.g calculation](../x.grad.png) ![W.g calculation](../W.grad.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_grad(x, w, b, y):\n",
    "    b.g = y.g.sum(dim=0)\n",
    "    w.g = x.T @ y.g\n",
    "    x.g = y.g @ w.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of a fully connected neural network with single hidden layer could be represented as follows:\n",
    "\n",
    "![NN with one hidden layer](../nn.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "215.434px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
