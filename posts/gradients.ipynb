{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Gradients, broadcasting and backpropagation\"\n",
    "author: \"Krasin Georgiev\"\n",
    "date: \"2022-11-23\"\n",
    "categories: [NN]\n",
    "format:\n",
    "  html:\n",
    "    code-fold: false\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have $y = w \\cdot x  + b$. Application of the rules of differentiation is simple, e.g. \n",
    "\n",
    "$$\\frac{\\mathrm{d}y}{\\mathrm{d}x} = w , \\quad\n",
    "\\frac{\\mathrm{d}y}{\\mathrm{d}w} = x, \\quad\n",
    "\\frac{\\mathrm{d}y}{\\mathrm{d}b} = 1$$\n",
    "\n",
    "The change in $y$ is porportional to the change in $x$. The bigger is $w$, the bigger is the change of $y$ for the same change of $x$.\n",
    "\n",
    "Let's $L = f(y) = f(y(x))$. Application of the chain rule is simple, e.g. \n",
    "\n",
    "$$\\frac{\\mathrm{d}L}{\\mathrm{d}x} = \\frac{\\mathrm{d}f(y)}{\\mathrm{d}y} \\cdot \\frac{\\mathrm{d}y}{\\mathrm{d}x} = \\frac{\\mathrm{d}L}{\\mathrm{d}y} \\cdot w$$ \n",
    "$$\\frac{\\mathrm{d}L}{\\mathrm{d}w} = \\frac{\\mathrm{d}f(y)}{\\mathrm{d}y} \\cdot \\frac{\\mathrm{d}y}{\\mathrm{d}w} = \\frac{\\mathrm{d}L}{\\mathrm{d}y} \\cdot x$$ \n",
    "$$\\frac{\\mathrm{d}L}{\\mathrm{d}b} = \\frac{\\mathrm{d}f(y)}{\\mathrm{d}y} \\cdot \\frac{\\mathrm{d}y}{\\mathrm{d}b} = \\frac{\\mathrm{d}L}{\\mathrm{d}y} \\cdot 1$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multidimensional case is not so simple. Functions with multiple inputs and multiple outputs have multiple partial derivatives which need to be arranged and stored properly. Applying this for batches of data complicates the picture even more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative of a transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of a function (transformation $\\psi$) with multiple inputs $\\mathbf{x} \\in \\mathbb{R}^M$ and multiple outputs $\\mathbf{y} \\in \\mathbb{R}^H$ is a matrix containing the partial derivatives of each output with respect to each input (the so called Jacobian of the transformation, $\\frac{\\partial{\\mathbf{y}}}{\\partial{\\mathbf{x}}} \\in \\mathbb{R}^{M \\times H}$). For example, if $M=4$ and $H=2$ we can write:\n",
    "\n",
    "$\\mathbf{y} = \\psi(\\mathbf{x})$,\n",
    "\n",
    "$\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4\\end{bmatrix}, \\quad\n",
    "\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix}$\n",
    "\n",
    "$\\frac{\\partial{\\mathbf{y}}}{\\partial{\\mathbf{x}}} = \\begin{bmatrix} \\frac{\\partial{y_1}}{\\partial{x_1}} & \\frac{\\partial{y_1}}{\\partial{x_2}} & \\frac{\\partial{y_1}}{\\partial{x_3}}  & \\frac{\\partial{y_1}}{\\partial{x_4}}\\\\\n",
    "\\frac{\\partial{y_2}}{\\partial{x_1}} & \\frac{\\partial{y_2}}{\\partial{x_2}} & \\frac{\\partial{y_2}}{\\partial{x_3}} & \\frac{\\partial{y_2}}{\\partial{x_4}}\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of $\\mathbf{y = W \\cdot x}$ with respect to $\\mathbf{x}$ is $\\mathbf{W}$, i.e. $\\frac{∂y}{∂x} = W$. \n",
    "\n",
    "We should note that in neural networks the input and output features are arranged as raw vectors.\n",
    "\n",
    "The derivative of $\\mathbf{y = x \\cdot W}$ with respect to $\\mathbf{x}$ is $\\mathbf{W^T}$, i.e. \n",
    "\n",
    "$$\\frac{∂y}{∂x} = W^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have $y = x \\cdot W + b$ and $L = f(y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Simple NN](../nn-mini.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chain rule involves propagating the gradient of the loss layer after layer backward towards the inputs and parameters of interest. In our demonstration case in order to calculate the gradient of $L$ with respect to the input $x$ we need to have the gradient of $L$ with respect to the output $y$. \n",
    "\n",
    "The shapes of the gradient is the same as the shape of the corresponding variable (parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A gradient is attached to each variable and parameter of the model**, i.e.\n",
    "\n",
    "$y.g = \\frac{\\partial{L}}{∂{y}}$\n",
    "\n",
    "$x.g = \\frac{\\partial{L}}{∂{x}} = \\frac{\\partial{L}}{\\partial{y}} \\cdot \\frac{\\partial{y}}{\\partial{x}} = y.g \\cdot \\frac{\\partial{y}}{\\partial{x}} = y.g \\cdot W^T$\n",
    "\n",
    "$b.g = \\frac{\\partial{L}}{∂{b}} = \\frac{\\partial{L}}{\\partial{y}} \\cdot \\frac{\\partial{y}}{\\partial{b}} = y.g \\cdot \\frac{\\partial{y}}{\\partial{b}} = y.g$\n",
    "\n",
    "$W.g = \\frac{\\partial{L}}{∂W} = ((\\frac{\\partial{L}}{\\partial{y}})^T \\cdot \\frac{\\partial{y}}{\\partial{W}})^T = (y.g^T \\cdot x)^T = x^T \\cdot y.g$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- $x$, $y$ and $b$ are row vectors.\n",
    "- $W$ is a weight matrix with $m$ rows and $h$ columns; \n",
    "- $x$ includes the $m$ input features\n",
    "- $b$ is a bias with $h$ elements; \n",
    "- $y$ has $h$ features (or nodes).\n",
    "- $x$ and $y$ represent input and output features (variables, nodes in the NN). Adding additional dimension (multiple rows) could represent multiple data samples. Inputs and outputs could be replaced by matrices $X$ and $Y$ where the last dimension gives the features ($x$ and $y$ for the corresponding data point); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Output y calculation](../xW.png) \n",
    "![x.g calculation](../x.grad.png) ![W.g calculation](../W.grad.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation and testing with code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above equations are implemented in function `lin_grad` and tested in the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_grad(x, w, b, y):\n",
    "    # y.g shoudl be available!\n",
    "    b.g = y.g.sum(dim=0)#/y.shape[0]\n",
    "    w.g = x.T @ y.g\n",
    "    x.g = y.g @ w.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.0000,  1.0000],\n",
       "         [ 1.5000,  1.5000],\n",
       "         [ 2.0000,  3.0000],\n",
       "         [ 2.5000, -0.5000]]),\n",
       " tensor([[ 11.8067,  -2.0911],\n",
       "         [ -9.3647,  -6.7345],\n",
       "         [  3.9591,  -7.5934],\n",
       "         [  9.4930,   2.5319],\n",
       "         [  2.0966,  13.3945],\n",
       "         [-12.2224, -16.5551],\n",
       "         [ 15.3866,   2.6479]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate test input and ouput data\n",
    "N = 100 # Number of samples\n",
    "M = 4 # Number of input features\n",
    "H = 2 # Number of outputs\n",
    "x = torch.rand((N, M))*10 - 5 # Input\n",
    "k1 = 1\n",
    "k2 = 1.5\n",
    "k3 = 2\n",
    "k4 = 2.5\n",
    "k6 = 3.0\n",
    "k7 = -0.5\n",
    "W_true = torch.tensor([[k1, k1],\n",
    "                  [k2, k2],\n",
    "                  [k3, k6],\n",
    "                  [k4, k7]])\n",
    "b_true = torch.tensor([0, -1])\n",
    "y = x @ W_true + b_true\n",
    "W_true, y[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f15e44674c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x[:,2], y[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.7646,  1.0763],\n",
       "         [ 3.4034, -0.4162],\n",
       "         [-0.9498,  0.2595],\n",
       "         [-0.4765,  0.1934]]),\n",
       " tensor([0., 0.]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate random weights\n",
    "w = torch.randn(M,H)\n",
    "b = torch.zeros(H)\n",
    "w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test the function `lin_grad` we need to calculate the gradient $∂L/dy$ and save it in $y.g$:\n",
    " - define model and calculate prediction\n",
    " - define and calculate loss $L$ as simple mean squared error `mse(y_pred, y_targ)`\n",
    " - define and run `mse_grad(y_pred, y_targ)`\n",
    " - run `lin_grad(x, w, b, y)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin(x, w, b): return x @ w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-10.3447,  -0.6232],\n",
       "        [-10.6888,  -4.0713],\n",
       "        [  0.3219,   2.6095],\n",
       "        [-10.7518,   5.9173],\n",
       "        [ 13.8666,  -1.1922]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = lin(x, w, b)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_pred, y_targ): return (y_pred-y_targ).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(137.9371)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mse(y_pred, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_grad(y_pred, y_targ): y_pred.g = 2 * (y_pred - y_targ) / y_targ.shape[0] / y_targ.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_grad(y_pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finnally, test if all dimensions in `lin_grad` match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_grad(x, w, b, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next test if the loss improves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.1110)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w -= 0.1 * w.g\n",
    "b -= 0.1 * b.g\n",
    "y_pred = lin(x, w, b)\n",
    "loss = mse(y_pred, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.1110)\n",
      "tensor(4.5152)\n",
      "tensor(3.9969)\n",
      "tensor(3.5457)\n",
      "tensor(3.1527)\n",
      "tensor(2.8100)\n",
      "tensor(2.5109)\n",
      "tensor(2.2498)\n",
      "tensor(2.0215)\n",
      "tensor(1.8218)\n",
      "tensor(1.6469)\n",
      "tensor(1.4935)\n",
      "tensor(1.3589)\n",
      "tensor(1.2406)\n",
      "tensor(1.1365)\n",
      "tensor(1.0448)\n",
      "tensor(0.9638)\n",
      "tensor(0.8923)\n",
      "tensor(0.8289)\n",
      "tensor(0.7728)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.9728,  0.9178],\n",
       "         [ 1.4296,  1.2903],\n",
       "         [ 1.8706,  2.7733],\n",
       "         [ 2.4438, -0.5472]]),\n",
       " tensor([-0.0191, -0.0628]),\n",
       " tensor([[ 1.0000,  1.0000],\n",
       "         [ 1.5000,  1.5000],\n",
       "         [ 2.0000,  3.0000],\n",
       "         [ 2.5000, -0.5000]]),\n",
       " tensor([ 0, -1]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    y_pred = lin(x, w, b)\n",
    "    loss = mse(y_pred, y)\n",
    "    mse_grad(y_pred, y)\n",
    "    lin_grad(x, w, b, y_pred)\n",
    "    w -= 0.01 * w.g\n",
    "    b -= 0.01 * b.g\n",
    "    print(loss)\n",
    "w, b, W_true, b_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch autograd and backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use PyTorch automatic gradient calculation to check our algorithms. This involves using the build-in methods and parameters `.backward()` and `.grad`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to apply PyTorch backpropagation and autograd we need to define a `forward` function that relates the inputs with the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, y):\n",
    "    y_pred = lin(x, w, b)\n",
    "    loss =  mse(y_pred, y)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7228)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = forward(x, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this is not enough: \n",
    "\n",
    "```\n",
    "    loss.backward()\n",
    "``` \n",
    "--> `RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn`\n",
    "\n",
    "Looks like a slot for saving the gradient to the corresponding inputs and model parameters should be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update gradients as above - manual backward - for comparison\n",
    "\n",
    "# forward\n",
    "y_pred = lin(x, w, b)\n",
    "loss =  mse(y_pred, y)\n",
    "\n",
    "# backward\n",
    "mse_grad(y_pred, y)\n",
    "lin_grad(x, w, b, y_pred)\n",
    "\n",
    "# Good to know: the parameters are not updated in the backward pass!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Require gradient to keep them with the data\n",
    "\n",
    "for element in [x, w, b]:\n",
    "    element.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = forward(x, y)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7228, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.test import test_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(x.g, x.grad)\n",
    "test_close(w.g, w.grad)\n",
    "test_close(b.g, b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to try to create a \"proper\" non-linear neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of a fully connected neural network with single hidden layer could be represented as follows:\n",
    "\n",
    "![NN with one hidden layer](../nn.png)\n",
    "\n",
    "A few more gradients need to be defined and calculated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "215.434px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
