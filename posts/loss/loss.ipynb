{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Loss\"\n",
    "author: \"Krasin Georgiev\"\n",
    "date: \"2022-12-3\"\n",
    "draft: true\n",
    "categories: [NN]\n",
    "format:\n",
    "  html:\n",
    "    code-fold: false\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax and Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let' define $z$ as the output of the last linear layer (no activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output $z$ can be converted to probability like values $\\hat{y}$ in two ways:\n",
    "\n",
    "- through sigmoid, $\\hat{y}_i = p(c_i) = \\mathrm{sigmoid}(z) = \\frac{1}{1 + \\exp(-z_i)} = \\frac{\\exp(z_i)}{\\exp(z_i)+\\exp(0)}$ - different outputs are independent, used for binnary classifier, could be used for multilabel-multiclass categorisation\n",
    "- through softmax, $\\hat{y}_i = p(c_i) = \\mathrm{softmax}(z) = \\frac{\\exp(z_i)}{\\sum{\\exp(z_j)}}$ - all outputs sum to one, used for multiclass categorisation\n",
    "\n",
    "where\n",
    "\n",
    "- $c_i$ is the category assigned to the $i$-th output node\n",
    "- $\\hat{y_i}$ is the estimated likelihood of $c_i$\n",
    "- $y_i$ is 1 if the actual category is $c_i$ and 0 otherwise. An alternative is to have $y$ as an integer representing the index of the actual category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ['male', 'female']\n",
    "z = tensor([0, 5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    # print('Input shape:', x.shape, 'Sum shape:', torch.exp(x).sum(dim=-1, keepdim=True).shape )\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.0000, 10.0000],\n",
       "         [ 2.0000, -2.0000],\n",
       "         [ 2.0000,  2.0000],\n",
       "         [ 0.0000,  2.0000],\n",
       "         [ 4.5000,  5.0000],\n",
       "         [ 0.0000,  0.0000]]),\n",
       " torch.Size([6, 2]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate test data - outputs of the last linear layer\n",
    "# Six items (N=6) and two output features (H=2)\n",
    "z = tensor([[1, 10],\n",
    "           [2, -2],\n",
    "           [2, 2],\n",
    "           [0, 2],\n",
    "           [4.5, 5],\n",
    "           [0, 0]\n",
    "           ])\n",
    "z, z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2339e-04, 9.9988e-01],\n",
       "        [9.8201e-01, 1.7986e-02],\n",
       "        [5.0000e-01, 5.0000e-01],\n",
       "        [1.1920e-01, 8.8080e-01],\n",
       "        [3.7754e-01, 6.2246e-01],\n",
       "        [5.0000e-01, 5.0000e-01]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.test import test_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(softmax(z), torch.softmax(z, dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crossentropy loss can be defined for binnary cases as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbb{L} = - \\sum_{i=1}^N{[ y_i \\ln(\\hat{y_i}) + (1 -y_i) \\ln(1 -\\hat{y}_i)]}$\n",
    "\n",
    "$\\mathbb{L} = - \\sum_{i=1}^N{[ y_i \\ln(p(c_i)) + (1 -y_i) \\ln(1 -p(c_i))]}$\n",
    "\n",
    "$\\mathbb{L} = - \\sum_{i=1}^N{[ y_i \\ln(\\mathrm{sigmoid(x_i)}) + (1 -y_i) \\ln(1 -\\mathrm{sigmoid(x_i)})]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crossentropy loss can be defined for multiclass cases as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbb{L} = - \\sum_{i=1}^N \\sum_{j=1}^K{y_{ij}\\ln(\\hat{y}_{ij}) }$ \n",
    "\n",
    "$\\mathbb{L} = - \\sum_{i=1}^N \\sum_{j=1}^K{y_{ij} \\ln(p(c_{ij})) }$\n",
    "\n",
    "$\\mathbb{L} = - \\sum_{i=1}^N \\sum_{j=1}^K{y_{ij} \\ln(\\mathrm{softmax(x_{ij})}) }$\n",
    "\n",
    "$\\mathbb{L} = - \\sum_{i=1}^N {\\ln(\\hat{y}_{i}) } \n",
    "= - \\sum_{i=1}^N { \\ln(p(c_{i})) } \n",
    "= - \\sum_{i=1}^N { \\ln(\\mathrm{softmax(x_{i})}) }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that:\n",
    "- Only the softmax of the true classes is needes as the other outputs are multiplied by zero ($y_ij=0$ for one hot encoded class different than $y_i$)\n",
    "- We need logarithm of the softmax, so the expression contain $\\log(\\exp())$ and can be simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    '''Logarithm of predicted probabilities calculated from the output'''\n",
    "    return softmax(x).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(log_softmax(w1), torch.log_softmax(w1, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax2(x):\n",
    "    return x - x.exp().sum(dim=-1, keepdim=True).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(log_softmax(w1), log_softmax2(w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x):\n",
    "    # a = x.max(dim=-1, keepdim=True)[0]\n",
    "    # return a + (x-a).exp().sum(dim=-1, keepdim=True).log()\n",
    "    a = x.max(dim=-1)[0]\n",
    "    return a + (x-a[...,None]).exp().sum(dim=-1).log()\n",
    "\n",
    "def log_softmax3(x):\n",
    "    # print(x.shape, logsumexp(x).shape)\n",
    "    return x - logsumexp(x).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(logsumexp(w1), torch.logsumexp(w1, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
