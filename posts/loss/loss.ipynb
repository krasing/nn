{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Loss\"\n",
    "author: \"Krasin Georgiev\"\n",
    "date: \"2022-12-5\"\n",
    "image: \"y_pred_vs_y.png\"\n",
    "categories: [NN]\n",
    "format:\n",
    "  html:\n",
    "    code-fold: false\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: clarify notations (not consistent yet!), add visualizations, add multi-label case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let' define $z$ as the output of the last linear layer (no activation):\n",
    "\n",
    "$z = [z_1, \\dots, z_h, \\dots, z_H]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the loss can be calculated some non-linear transformations may be needed. For example, a classification problem requires the output to be interpreted as a probability for the class assigned to the output node. Therefore the output is skewed to fit in the range [0, 1].\n",
    "\n",
    "It should be noted that this need is not limited to the classification tasks. In a regression problem for predicting values with high dynamic ranges, the same error does not have the same effect for all predictions, e.g. prediction error of \\\\$1 for a product of \\\\$1000 is small and acceptable, but the same error when the product is \\\\$2 is not good. The simplest solution in such situation is to use logarithm, but this is out of the scope of this tutorial.\n",
    "\n",
    "The standard loss functions are crossentropy (for classification) and mean squared error (for regression). The non-linearity could be added as a separate layer or could be part of the Loss calculation. This changes the actual output and loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid and softmax non-linearities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output $z$ can be converted to probability like values $\\hat{y}$ in two ways:\n",
    "\n",
    "- through sigmoid, $\\hat{y}_h = p(c_h) = \\mathrm{sigmoid}(z) = \\frac{1}{1 + \\exp(-z_h)} = \\frac{\\exp(z_h)}{\\exp(z_h)+\\exp(0)}$ - different outputs are independent, used for binnary classifier, could be used for multilabel-multiclass categorisation\n",
    "- through softmax, $\\hat{y}_h = p(c_h) = \\mathrm{softmax}(z) = \\frac{\\exp(z_h)}{\\sum{\\exp(z_j)}}$ - all outputs sum to one, used for multiclass categorisation\n",
    "\n",
    "where\n",
    "\n",
    "- $c_h$ is the category assigned to the $h$-th output node\n",
    "- $\\hat{y_h}$ is the estimated likelihood of $c_h$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available categories\n",
    "c = ['male', 'female']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 5.]), torch.Size([2]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate test data - example output z of the last linear layer\n",
    "# Two output features (H=2)\n",
    "z = tensor([0., 5.])\n",
    "z, z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    # print('Input shape:', x.shape, 'Sum shape:', torch.exp(x).sum(dim=-1, keepdim=True).shape )\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5000, 0.9933]), tensor([0.0067, 0.9933]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(z), softmax(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.test import test_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(softmax(z), torch.softmax(z, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(sigmoid(z), torch.sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are designed to process data in batches. This means that the input (and the output) will have one additional dimension for the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.0000, 10.0000],\n",
       "         [ 2.0000, -2.0000],\n",
       "         [ 2.0000,  2.0000],\n",
       "         [ 0.0000,  2.0000],\n",
       "         [ 4.5000,  5.0000],\n",
       "         [ 0.0000,  0.0000]]),\n",
       " torch.Size([6, 2]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate test data - batch outputs of the last linear layer\n",
    "# Six items (N=6) and two output features (H=2)\n",
    "zz = tensor([[1, 10],\n",
    "           [2, -2],\n",
    "           [2, 2],\n",
    "           [0, 2],\n",
    "           [4.5, 5],\n",
    "           [0, 0]\n",
    "           ])\n",
    "zz, zz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(sigmoid(zz), torch.sigmoid(zz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(softmax(zz), torch.softmax(zz, dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossentropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true classes/labels are needed in addition to the model predictions in order to calculate the loss.\n",
    "\n",
    "Let's denote:\n",
    "\n",
    "- $y_{ih}$ is 1 if for sample $i$ the true class is $c_h$ and 0 otherwise.\n",
    "- $y_i = \\mathrm{argmax}(y_{ih})$ is the index of the true class for sample $i$ from the batch.\n",
    "- $\\hat{y}_{ih}$ is the estimated likelihood of $c_h$ for sample $i$\n",
    "- $\\hat{y}_{i} = \\hat{y}[i, j=\\mathrm{argmax}(y_{ih})]$ is the estimated likelihood of the true class for sample $i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crossentropy loss can be defined for binnary cases as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbb{L} = - \\sum_{i=1}^N{[ y_i \\ln(\\hat{y_i}) + (1 -y_i) \\ln(1 -\\hat{y}_i)]}$\n",
    "\n",
    "$\\mathbb{L} = - \\sum_{i=1}^N{[ y_i \\ln(p(c_i)) + (1 -y_i) \\ln(1 -p(c_i))]}$\n",
    "\n",
    "$\\mathbb{L} = - \\sum_{i=1}^N{[ y_i \\ln(\\mathrm{sigmoid(x_i)}) + (1 -y_i) \\ln(1 -\\mathrm{sigmoid(x_i)})]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crossentropy loss can be defined for multiclass cases as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbb{L} = - \\sum_{i=1}^N \\sum_{h=1}^H{y_{ij}\\ln(\\hat{y}_{ih}) }$ \n",
    "\n",
    "$\\mathbb{L} = - \\sum_{i=1}^N \\sum_{h=1}^H{y_{ij} \\ln(p(c_{ih})) }$\n",
    "\n",
    "$\\mathbb{L} = - \\sum_{i=1}^N \\sum_{h=1}^H{y_{ij} \\ln(\\mathrm{softmax(x_{ih})}) }$\n",
    "\n",
    "$\\mathbb{L} = - \\sum_{i=1}^N {\\ln(\\hat{y}_{i}) } \n",
    "= - \\sum_{i=1}^N { \\ln(p(c_{i})) } \n",
    "= - \\sum_{i=1}^N { \\ln(\\mathrm{softmax(x_{i})}) }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that:\n",
    "\n",
    "- Only the softmax of the true classes is needes as the other outputs are multiplied by zero ($y_{ij}=0$ for one hot encoded class different than $y_i$)\n",
    "- We need logarithm of the softmax, so the expression contain $\\log(\\exp())$ and can be simplified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log(Softmax) calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    '''Logarithm of predicted probabilities calculated from the output'''\n",
    "    return softmax(x).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(log_softmax(z), torch.log_softmax(z, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(log_softmax(zz), torch.log_softmax(zz, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax2(x):\n",
    "    return x - x.exp().sum(dim=-1, keepdim=True).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(log_softmax2(z), torch.log_softmax(z, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(log_softmax2(zz), torch.log_softmax(zz, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x):\n",
    "    # a = x.max(dim=-1, keepdim=True)[0]\n",
    "    # return a + (x-a).exp().sum(dim=-1, keepdim=True).log()\n",
    "    a = x.max(dim=-1)[0]\n",
    "    return a + (x-a[...,None]).exp().sum(dim=-1).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(logsumexp(z), torch.logsumexp(z, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(logsumexp(zz), torch.logsumexp(zz, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax3(x):\n",
    "    return x - logsumexp(x).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(log_softmax3(z), torch.log_softmax(z, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(log_softmax3(zz), torch.log_softmax(zz, dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-entropy loss for log-probabilities, `F.nll_loss()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbb{L} = - \\sum_{i=1}^N \\sum_{h=1}^H{y_{ij} \\ln(p(c_{ih})) } = - \\sum_{i=1}^N {\\ln(\\hat{y}_{i}) }$\n",
    "\n",
    "If case of training with such a loss function, the output of the network should be interpreted as log-probabilities. To convert to probabilities, take the exponent of the predictions. My note: a kind of failure intensity: $p = \\exp(-\\lambda t) \\implies \\ln(p) = -\\lambda t$ compare with $q = 1 - p = 1 - \\exp(-\\lambda t) \\approx \\lambda t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(x, y):\n",
    "    '''Take the mean value of the correct x\n",
    "       x: pred_as_log_softmax\n",
    "       y: target_as_index\n",
    "    '''\n",
    "    N = y.shape[0]\n",
    "    loss = -x[range(N), y].mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.1568)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vv = -torch.rand((6, 2))*5\n",
    "y = torch.randint(0, 2, (6,))\n",
    "nll(vv, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.5796, -1.7065],\n",
       "         [-3.8344, -3.1713],\n",
       "         [-2.9748, -4.5742],\n",
       "         [-2.5904, -3.6987],\n",
       "         [-1.0223, -3.4777],\n",
       "         [-4.0273, -1.6493]]),\n",
       " tensor([1, 0, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# notice that the log-likelihood values are negative!\n",
    "vv, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(nll(vv,y), F.nll_loss(vv, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-4.00', '-3.00', '-2.00', '-1.00', '-0.50', '-0.20', '-0.05', '0.00', '0.50']\n",
      "['0.018', '0.050', '0.135', '0.368', '0.607', '0.819', '0.951', '1.000', '1.649']\n"
     ]
    }
   ],
   "source": [
    "a = tensor([-4, -3, -2, -1, -0.5, -0.2, -0.05, 0, 0.5])\n",
    "aa = a.exp()\n",
    "print(['%.2f' %i for i in a])\n",
    "print(['%.3f' %i for i in aa])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-entropy loss for raw outputs, `F.cross_entropy()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-entropy is calculated directly from the output without conversion to probabilities or log-probabilities. All these operations are included in the loss function calculation. The interpretation of the output is unclear, could be any value from $-\\infty$ to $\\infty$ â€“ useful for other regression tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(x, y):\n",
    "    return nll(log_softmax3(x), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3343)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(zz, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(cross_entropy(zz, y), F.cross_entropy(zz, y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
